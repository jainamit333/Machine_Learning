{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.0.0\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/mnist/mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>1x10</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  28x19  28x20  \\\n",
       "0    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "1    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "2    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "3    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "4    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[50000:].values.astype('float32')/255\n",
    "X_test = data[:50000].values.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (10000, 784)\n",
      "Test data shape (50000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape', X_train.shape)\n",
    "print('Test data shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common params\n",
    "epochs = 250\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Complete Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_hidden = 256\n",
    "n_output = n_input\n",
    "\n",
    "##input\n",
    "input_mg = tf.keras.layers.Input(shape=(n_input,))\n",
    "##hideen layer or coding layer\n",
    "encoded = tf.keras.layers.Dense(n_hidden, activation='relu')(input_mg)\n",
    "##output layer\n",
    "decoded = tf.keras.layers.Dense(n_output, activation='sigmoid')(encoded)\n",
    "##model\n",
    "autoencoder = tf.keras.models.Model(inputs=input_mg, outputs =decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperate encoder model\n",
    "encoder = tf.keras.models.Model(inputs=input_mg, outputs =encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperate decoder model\n",
    "encoded_input = tf.keras.layers.Input(shape=(n_hidden,))\n",
    "decoder_layer = autoencoder.layers[-1] ## last layer of autoencoder, can we use decoded directly\n",
    "decoder = tf.keras.models.Model(inputs=encoded_input, outputs =decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 50000 samples\n",
      "Epoch 1/250\n",
      "10000/10000 [==============================] - 3s 318us/sample - loss: 0.3582 - val_loss: 0.2345\n",
      "Epoch 2/250\n",
      "10000/10000 [==============================] - 2s 217us/sample - loss: 0.2003 - val_loss: 0.1781\n",
      "Epoch 3/250\n",
      "10000/10000 [==============================] - 2s 221us/sample - loss: 0.1607 - val_loss: 0.1516\n",
      "Epoch 4/250\n",
      "10000/10000 [==============================] - 2s 220us/sample - loss: 0.1396 - val_loss: 0.1347\n",
      "Epoch 5/250\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 0.1256 - val_loss: 0.1230\n",
      "Epoch 6/250\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 0.1156 - val_loss: 0.1145\n",
      "Epoch 7/250\n",
      "10000/10000 [==============================] - 2s 218us/sample - loss: 0.1081 - val_loss: 0.1082\n",
      "Epoch 8/250\n",
      "10000/10000 [==============================] - 2s 223us/sample - loss: 0.1023 - val_loss: 0.1028\n",
      "Epoch 9/250\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 0.0979 - val_loss: 0.0986\n",
      "Epoch 10/250\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 0.0942 - val_loss: 0.0953\n",
      "Epoch 11/250\n",
      "10000/10000 [==============================] - 2s 232us/sample - loss: 0.0910 - val_loss: 0.0923\n",
      "Epoch 12/250\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 0.0884 - val_loss: 0.0905\n",
      "Epoch 13/250\n",
      "10000/10000 [==============================] - 2s 236us/sample - loss: 0.0862 - val_loss: 0.0879\n",
      "Epoch 14/250\n",
      "10000/10000 [==============================] - 2s 237us/sample - loss: 0.0843 - val_loss: 0.0857\n",
      "Epoch 15/250\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 0.0825 - val_loss: 0.0840\n",
      "Epoch 16/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0811 - val_loss: 0.0833\n",
      "Epoch 17/250\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.0797 - val_loss: 0.0813\n",
      "Epoch 18/250\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 0.0786 - val_loss: 0.0801\n",
      "Epoch 19/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0775 - val_loss: 0.0790\n",
      "Epoch 20/250\n",
      "10000/10000 [==============================] - 2s 236us/sample - loss: 0.0766 - val_loss: 0.0782\n",
      "Epoch 21/250\n",
      "10000/10000 [==============================] - 2s 245us/sample - loss: 0.0758 - val_loss: 0.0774\n",
      "Epoch 22/250\n",
      "10000/10000 [==============================] - 2s 245us/sample - loss: 0.0751 - val_loss: 0.0765\n",
      "Epoch 23/250\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.0744 - val_loss: 0.0760\n",
      "Epoch 24/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0738 - val_loss: 0.0752\n",
      "Epoch 25/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0732 - val_loss: 0.0748\n",
      "Epoch 26/250\n",
      "10000/10000 [==============================] - 2s 237us/sample - loss: 0.0727 - val_loss: 0.0741\n",
      "Epoch 27/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0722 - val_loss: 0.0738\n",
      "Epoch 28/250\n",
      "10000/10000 [==============================] - 2s 236us/sample - loss: 0.0718 - val_loss: 0.0733\n",
      "Epoch 29/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0715 - val_loss: 0.0729\n",
      "Epoch 30/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0710 - val_loss: 0.0725\n",
      "Epoch 31/250\n",
      "10000/10000 [==============================] - 3s 299us/sample - loss: 0.0707 - val_loss: 0.0721\n",
      "Epoch 32/250\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0704 - val_loss: 0.0718\n",
      "Epoch 33/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0701 - val_loss: 0.0716\n",
      "Epoch 34/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0699 - val_loss: 0.0712\n",
      "Epoch 35/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0696 - val_loss: 0.0712\n",
      "Epoch 36/250\n",
      "10000/10000 [==============================] - 2s 234us/sample - loss: 0.0694 - val_loss: 0.0708\n",
      "Epoch 37/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0692 - val_loss: 0.0709\n",
      "Epoch 38/250\n",
      "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0690 - val_loss: 0.0707\n",
      "Epoch 39/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0688 - val_loss: 0.0701\n",
      "Epoch 40/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0685 - val_loss: 0.0700\n",
      "Epoch 41/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0684 - val_loss: 0.0698\n",
      "Epoch 42/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0682 - val_loss: 0.0697\n",
      "Epoch 43/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0681 - val_loss: 0.0694\n",
      "Epoch 44/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0680 - val_loss: 0.0693\n",
      "Epoch 45/250\n",
      "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0678 - val_loss: 0.0692\n",
      "Epoch 46/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0677 - val_loss: 0.0691\n",
      "Epoch 47/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0676 - val_loss: 0.0689\n",
      "Epoch 48/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0674 - val_loss: 0.0690\n",
      "Epoch 49/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0674 - val_loss: 0.0688\n",
      "Epoch 50/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0672 - val_loss: 0.0686\n",
      "Epoch 51/250\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0671 - val_loss: 0.0685\n",
      "Epoch 52/250\n",
      "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0670 - val_loss: 0.0683\n",
      "Epoch 53/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0669 - val_loss: 0.0683\n",
      "Epoch 54/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0668 - val_loss: 0.0682\n",
      "Epoch 55/250\n",
      "10000/10000 [==============================] - 2s 243us/sample - loss: 0.0667 - val_loss: 0.0682\n",
      "Epoch 56/250\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0667 - val_loss: 0.0680\n",
      "Epoch 57/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0666 - val_loss: 0.0680\n",
      "Epoch 58/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0665 - val_loss: 0.0679\n",
      "Epoch 59/250\n",
      "10000/10000 [==============================] - 2s 245us/sample - loss: 0.0664 - val_loss: 0.0679\n",
      "Epoch 60/250\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0664 - val_loss: 0.0679\n",
      "Epoch 61/250\n",
      "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0663 - val_loss: 0.0676\n",
      "Epoch 62/250\n",
      "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0662 - val_loss: 0.0676\n",
      "Epoch 63/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0662 - val_loss: 0.0677\n",
      "Epoch 64/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0661 - val_loss: 0.0675\n",
      "Epoch 65/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0660 - val_loss: 0.0674\n",
      "Epoch 66/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0660 - val_loss: 0.0674\n",
      "Epoch 67/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0659 - val_loss: 0.0673\n",
      "Epoch 68/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0659 - val_loss: 0.0673\n",
      "Epoch 69/250\n",
      "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0658 - val_loss: 0.0672\n",
      "Epoch 70/250\n",
      "10000/10000 [==============================] - 2s 250us/sample - loss: 0.0658 - val_loss: 0.0672\n",
      "Epoch 71/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0658 - val_loss: 0.0672\n",
      "Epoch 72/250\n",
      "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0657 - val_loss: 0.0670\n",
      "Epoch 73/250\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0656 - val_loss: 0.0670\n",
      "Epoch 74/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0656 - val_loss: 0.0671\n",
      "Epoch 75/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0656 - val_loss: 0.0670\n",
      "Epoch 76/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0655 - val_loss: 0.0669\n",
      "Epoch 77/250\n",
      "10000/10000 [==============================] - 2s 249us/sample - loss: 0.0655 - val_loss: 0.0669\n",
      "Epoch 78/250\n",
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0654 - val_loss: 0.0669\n",
      "Epoch 79/250\n",
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0654 - val_loss: 0.0668\n",
      "Epoch 80/250\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.0654 - val_loss: 0.0668\n",
      "Epoch 81/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0653 - val_loss: 0.0668\n",
      "Epoch 82/250\n",
      "10000/10000 [==============================] - 2s 243us/sample - loss: 0.0653 - val_loss: 0.0667\n",
      "Epoch 83/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0652 - val_loss: 0.0667\n",
      "Epoch 84/250\n",
      "10000/10000 [==============================] - 2s 242us/sample - loss: 0.0652 - val_loss: 0.0667\n",
      "Epoch 85/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0652 - val_loss: 0.0667\n",
      "Epoch 86/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0652 - val_loss: 0.0666\n",
      "Epoch 87/250\n",
      "10000/10000 [==============================] - 2s 243us/sample - loss: 0.0651 - val_loss: 0.0666\n",
      "Epoch 88/250\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 0.0651 - val_loss: 0.0665\n",
      "Epoch 89/250\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0650 - val_loss: 0.0665\n",
      "Epoch 90/250\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0650 - val_loss: 0.0665\n",
      "Epoch 91/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0650 - val_loss: 0.0664\n",
      "Epoch 92/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0650 - val_loss: 0.0665\n",
      "Epoch 93/250\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0649 - val_loss: 0.0665\n",
      "Epoch 94/250\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 0.0650 - val_loss: 0.0664\n",
      "Epoch 95/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0649 - val_loss: 0.0664\n",
      "Epoch 96/250\n",
      "10000/10000 [==============================] - 2s 240us/sample - loss: 0.0648 - val_loss: 0.0663\n",
      "Epoch 97/250\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 0.0648 - val_loss: 0.0663\n",
      "Epoch 98/250\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 0.0648 - val_loss: 0.0663\n",
      "Epoch 99/250\n",
      "10000/10000 [==============================] - 2s 242us/sample - loss: 0.0648 - val_loss: 0.0663\n",
      "Epoch 100/250\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.0648 - val_loss: 0.0662\n",
      "Epoch 101/250\n",
      "10000/10000 [==============================] - 2s 239us/sample - loss: 0.0647 - val_loss: 0.0663\n",
      "Epoch 102/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0647 - val_loss: 0.0665\n",
      "Epoch 103/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0647 - val_loss: 0.0662\n",
      "Epoch 104/250\n",
      "10000/10000 [==============================] - 2s 243us/sample - loss: 0.0647 - val_loss: 0.0662\n",
      "Epoch 105/250\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 0.0647 - val_loss: 0.0663\n",
      "Epoch 106/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0646 - val_loss: 0.0661\n",
      "Epoch 107/250\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0646 - val_loss: 0.0662\n",
      "Epoch 108/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0646 - val_loss: 0.0661\n",
      "Epoch 109/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0646 - val_loss: 0.0661\n",
      "Epoch 110/250\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0646 - val_loss: 0.0661\n",
      "Epoch 111/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0645 - val_loss: 0.0660\n",
      "Epoch 112/250\n",
      "10000/10000 [==============================] - 2s 243us/sample - loss: 0.0645 - val_loss: 0.0660\n",
      "Epoch 113/250\n",
      "10000/10000 [==============================] - 2s 242us/sample - loss: 0.0645 - val_loss: 0.0660\n",
      "Epoch 114/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0645 - val_loss: 0.0660\n",
      "Epoch 115/250\n",
      "10000/10000 [==============================] - 3s 263us/sample - loss: 0.0644 - val_loss: 0.0660\n",
      "Epoch 116/250\n",
      "10000/10000 [==============================] - 2s 245us/sample - loss: 0.0645 - val_loss: 0.0660\n",
      "Epoch 117/250\n",
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0644 - val_loss: 0.0661\n",
      "Epoch 118/250\n",
      "10000/10000 [==============================] - 3s 304us/sample - loss: 0.0644 - val_loss: 0.0660\n",
      "Epoch 119/250\n",
      "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0644 - val_loss: 0.0659\n",
      "Epoch 120/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0644 - val_loss: 0.0660\n",
      "Epoch 121/250\n",
      "10000/10000 [==============================] - 3s 292us/sample - loss: 0.0644 - val_loss: 0.0659\n",
      "Epoch 122/250\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0644 - val_loss: 0.0659\n",
      "Epoch 123/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0643 - val_loss: 0.0659\n",
      "Epoch 124/250\n",
      "10000/10000 [==============================] - 3s 280us/sample - loss: 0.0643 - val_loss: 0.0659\n",
      "Epoch 125/250\n",
      "10000/10000 [==============================] - 3s 291us/sample - loss: 0.0643 - val_loss: 0.0660\n",
      "Epoch 126/250\n",
      "10000/10000 [==============================] - 3s 277us/sample - loss: 0.0643 - val_loss: 0.0659\n",
      "Epoch 127/250\n",
      "10000/10000 [==============================] - 2s 242us/sample - loss: 0.0643 - val_loss: 0.0658\n",
      "Epoch 128/250\n",
      "10000/10000 [==============================] - 2s 236us/sample - loss: 0.0643 - val_loss: 0.0658\n",
      "Epoch 129/250\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 130/250\n",
      "10000/10000 [==============================] - 2s 248us/sample - loss: 0.0642 - val_loss: 0.0659\n",
      "Epoch 131/250\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 132/250\n",
      "10000/10000 [==============================] - 3s 293us/sample - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 133/250\n",
      "10000/10000 [==============================] - 2s 245us/sample - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 134/250\n",
      "10000/10000 [==============================] - 2s 244us/sample - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 135/250\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0642 - val_loss: 0.0658\n",
      "Epoch 136/250\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0642 - val_loss: 0.0657\n",
      "Epoch 137/250\n",
      "10000/10000 [==============================] - 3s 263us/sample - loss: 0.0642 - val_loss: 0.0659\n",
      "Epoch 138/250\n",
      "10000/10000 [==============================] - 3s 280us/sample - loss: 0.0642 - val_loss: 0.0657\n",
      "Epoch 139/250\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 140/250\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 141/250\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0641 - val_loss: 0.0658\n",
      "Epoch 142/250\n",
      "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 143/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 144/250\n",
      "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 145/250\n",
      "10000/10000 [==============================] - 3s 305us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 146/250\n",
      "10000/10000 [==============================] - 3s 299us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 147/250\n",
      "10000/10000 [==============================] - 4s 397us/sample - loss: 0.0641 - val_loss: 0.0657\n",
      "Epoch 148/250\n",
      "10000/10000 [==============================] - 3s 290us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 149/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 262us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 150/250\n",
      "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 151/250\n",
      "10000/10000 [==============================] - 3s 268us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 152/250\n",
      "10000/10000 [==============================] - 3s 283us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 153/250\n",
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 154/250\n",
      "10000/10000 [==============================] - 3s 265us/sample - loss: 0.0640 - val_loss: 0.0657\n",
      "Epoch 155/250\n",
      "10000/10000 [==============================] - 3s 279us/sample - loss: 0.0640 - val_loss: 0.0656\n",
      "Epoch 156/250\n",
      "10000/10000 [==============================] - 3s 276us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 157/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0639 - val_loss: 0.0657\n",
      "Epoch 158/250\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0640 - val_loss: 0.0656\n",
      "Epoch 159/250\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0640 - val_loss: 0.0656\n",
      "Epoch 160/250\n",
      "10000/10000 [==============================] - 3s 282us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 161/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 162/250\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 163/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 164/250\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 165/250\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 166/250\n",
      "10000/10000 [==============================] - 3s 261us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 167/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0639 - val_loss: 0.0655\n",
      "Epoch 168/250\n",
      "10000/10000 [==============================] - 2s 248us/sample - loss: 0.0639 - val_loss: 0.0656\n",
      "Epoch 169/250\n",
      "10000/10000 [==============================] - 3s 264us/sample - loss: 0.0639 - val_loss: 0.0655\n",
      "Epoch 170/250\n",
      "10000/10000 [==============================] - 3s 286us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 171/250\n",
      "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 172/250\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0638 - val_loss: 0.0656\n",
      "Epoch 173/250\n",
      "10000/10000 [==============================] - 2s 247us/sample - loss: 0.0638 - val_loss: 0.0656\n",
      "Epoch 174/250\n",
      "10000/10000 [==============================] - 3s 295us/sample - loss: 0.0638 - val_loss: 0.0656\n",
      "Epoch 175/250\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 0.0638 - val_loss: 0.0656\n",
      "Epoch 176/250\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 177/250\n",
      "10000/10000 [==============================] - 3s 307us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 178/250\n",
      "10000/10000 [==============================] - 3s 315us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 179/250\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 180/250\n",
      "10000/10000 [==============================] - 3s 270us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 181/250\n",
      "10000/10000 [==============================] - 3s 302us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 182/250\n",
      "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 183/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0638 - val_loss: 0.0657\n",
      "Epoch 184/250\n",
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0638 - val_loss: 0.0656\n",
      "Epoch 185/250\n",
      "10000/10000 [==============================] - 3s 282us/sample - loss: 0.0638 - val_loss: 0.0656\n",
      "Epoch 186/250\n",
      "10000/10000 [==============================] - 3s 266us/sample - loss: 0.0638 - val_loss: 0.0654\n",
      "Epoch 187/250\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0638 - val_loss: 0.0655\n",
      "Epoch 188/250\n",
      "10000/10000 [==============================] - 3s 297us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 189/250\n",
      "10000/10000 [==============================] - 3s 305us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 190/250\n",
      "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 191/250\n",
      "10000/10000 [==============================] - 3s 269us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 192/250\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 193/250\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.0637 - val_loss: 0.0654\n",
      "Epoch 194/250\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 195/250\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 196/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0637 - val_loss: 0.0656\n",
      "Epoch 197/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 198/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 199/250\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 200/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 201/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 202/250\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0637 - val_loss: 0.0654\n",
      "Epoch 203/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0637 - val_loss: 0.0654\n",
      "Epoch 204/250\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 205/250\n",
      "10000/10000 [==============================] - 3s 267us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 206/250\n",
      "10000/10000 [==============================] - 3s 258us/sample - loss: 0.0637 - val_loss: 0.0655\n",
      "Epoch 207/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0636 - val_loss: 0.0656\n",
      "Epoch 208/250\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 209/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 210/250\n",
      "10000/10000 [==============================] - 2s 248us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 211/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 212/250\n",
      "10000/10000 [==============================] - 3s 259us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 213/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 214/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 215/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 216/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 217/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 218/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 219/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 220/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 221/250\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 222/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0636 - val_loss: 0.0654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 224/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 225/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 226/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 227/250\n",
      "10000/10000 [==============================] - 2s 250us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 228/250\n",
      "10000/10000 [==============================] - 3s 274us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 229/250\n",
      "10000/10000 [==============================] - 3s 272us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 230/250\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 231/250\n",
      "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0636 - val_loss: 0.0654\n",
      "Epoch 232/250\n",
      "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 233/250\n",
      "10000/10000 [==============================] - 3s 287us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 234/250\n",
      "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0635 - val_loss: 0.0655\n",
      "Epoch 235/250\n",
      "10000/10000 [==============================] - 4s 393us/sample - loss: 0.0636 - val_loss: 0.0656\n",
      "Epoch 236/250\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 0.0636 - val_loss: 0.0655\n",
      "Epoch 237/250\n",
      "10000/10000 [==============================] - 4s 366us/sample - loss: 0.0635 - val_loss: 0.0653\n",
      "Epoch 238/250\n",
      "10000/10000 [==============================] - 3s 310us/sample - loss: 0.0635 - val_loss: 0.0655\n",
      "Epoch 239/250\n",
      "10000/10000 [==============================] - 3s 292us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 240/250\n",
      "10000/10000 [==============================] - 3s 291us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 241/250\n",
      "10000/10000 [==============================] - 3s 271us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 242/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 243/250\n",
      "10000/10000 [==============================] - 3s 260us/sample - loss: 0.0635 - val_loss: 0.0653\n",
      "Epoch 244/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0635 - val_loss: 0.0655\n",
      "Epoch 245/250\n",
      "10000/10000 [==============================] - 3s 253us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 246/250\n",
      "10000/10000 [==============================] - 3s 251us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 247/250\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 0.0635 - val_loss: 0.0653\n",
      "Epoch 248/250\n",
      "10000/10000 [==============================] - 3s 250us/sample - loss: 0.0635 - val_loss: 0.0655\n",
      "Epoch 249/250\n",
      "10000/10000 [==============================] - 3s 263us/sample - loss: 0.0635 - val_loss: 0.0654\n",
      "Epoch 250/250\n",
      "10000/10000 [==============================] - 3s 254us/sample - loss: 0.0635 - val_loss: 0.0654\n"
     ]
    }
   ],
   "source": [
    "# now let's train our autoencoder for 50 epochs\n",
    "result_ucla = autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, \n",
    "                shuffle=True, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(X_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now using Matplotlib to plot the images\n",
    "n = 10 # how many images we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now using Matplotlib to plot the images\n",
    "n = 10 # how many images we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(encoded_imgs[i].reshape(16, 16))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_hidden_1 = 484\n",
    "n_hidden_2 = 256\n",
    "n_hidden_3 = 81\n",
    "n_output = n_input\n",
    "\n",
    "##input\n",
    "input_mg = tf.keras.layers.Input(shape=(n_input,))\n",
    "##hideen layer or coding layer\n",
    "encoded_1 = tf.keras.layers.Dense(n_hidden_1, activation='relu')(input_mg)\n",
    "encoded_2 = tf.keras.layers.Dense(n_hidden_2, activation='relu')(encoded_1)\n",
    "encoded_3 = tf.keras.layers.Dense(n_hidden_3, activation='relu')(encoded_2)\n",
    "\n",
    "##output layer\n",
    "decoded_2 = tf.keras.layers.Dense(n_hidden_2, activation='sigmoid')(encoded_3)\n",
    "decoded_1 = tf.keras.layers.Dense(n_hidden_1, activation='sigmoid')(decoded_2)\n",
    "output_mg = tf.keras.layers.Dense(n_output, activation='sigmoid')(decoded_1)\n",
    "##model\n",
    "autoencoder = tf.keras.models.Model(inputs=input_mg, outputs =output_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 50000 samples\n",
      "Epoch 1/250\n",
      "10000/10000 [==============================] - 5s 514us/sample - loss: 0.3167 - val_loss: 0.2650\n",
      "Epoch 2/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.2584 - val_loss: 0.2535\n",
      "Epoch 3/250\n",
      "10000/10000 [==============================] - 4s 441us/sample - loss: 0.2462 - val_loss: 0.2453\n",
      "Epoch 4/250\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 0.2391 - val_loss: 0.2354\n",
      "Epoch 5/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2257 - val_loss: 0.2205\n",
      "Epoch 6/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.2121 - val_loss: 0.2113\n",
      "Epoch 7/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.2042 - val_loss: 0.2021\n",
      "Epoch 8/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.1936 - val_loss: 0.1905\n",
      "Epoch 9/250\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.1842 - val_loss: 0.1841\n",
      "Epoch 10/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.1795 - val_loss: 0.1806\n",
      "Epoch 11/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.1760 - val_loss: 0.1770\n",
      "Epoch 12/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.1723 - val_loss: 0.1729\n",
      "Epoch 13/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1684 - val_loss: 0.1694\n",
      "Epoch 14/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1641 - val_loss: 0.1638\n",
      "Epoch 15/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1588 - val_loss: 0.1584\n",
      "Epoch 16/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1543 - val_loss: 0.1545\n",
      "Epoch 17/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.1508 - val_loss: 0.1508\n",
      "Epoch 18/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.1474 - val_loss: 0.1483\n",
      "Epoch 19/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.1447 - val_loss: 0.1455\n",
      "Epoch 20/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.1422 - val_loss: 0.1434\n",
      "Epoch 21/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.1399 - val_loss: 0.1414\n",
      "Epoch 22/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1379 - val_loss: 0.1391\n",
      "Epoch 23/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.1356 - val_loss: 0.1371\n",
      "Epoch 24/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.1336 - val_loss: 0.1351\n",
      "Epoch 25/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.1314 - val_loss: 0.1329\n",
      "Epoch 26/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.1294 - val_loss: 0.1317\n",
      "Epoch 27/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1276 - val_loss: 0.1291\n",
      "Epoch 28/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.1259 - val_loss: 0.1283\n",
      "Epoch 29/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1244 - val_loss: 0.1266\n",
      "Epoch 30/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.1229 - val_loss: 0.1249\n",
      "Epoch 31/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1214 - val_loss: 0.1237\n",
      "Epoch 32/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.1200 - val_loss: 0.1226\n",
      "Epoch 33/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.1186 - val_loss: 0.1208\n",
      "Epoch 34/250\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 0.1172 - val_loss: 0.1199\n",
      "Epoch 35/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.1161 - val_loss: 0.1180\n",
      "Epoch 36/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.1145 - val_loss: 0.1173\n",
      "Epoch 37/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.1135 - val_loss: 0.1160\n",
      "Epoch 38/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.1125 - val_loss: 0.1150\n",
      "Epoch 39/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.1114 - val_loss: 0.1140\n",
      "Epoch 40/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1104 - val_loss: 0.1132\n",
      "Epoch 41/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.1094 - val_loss: 0.1128\n",
      "Epoch 42/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.1084 - val_loss: 0.1110\n",
      "Epoch 43/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1074 - val_loss: 0.1102\n",
      "Epoch 44/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.1063 - val_loss: 0.1092\n",
      "Epoch 45/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1054 - val_loss: 0.1093\n",
      "Epoch 46/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1046 - val_loss: 0.1085\n",
      "Epoch 47/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.1037 - val_loss: 0.1068\n",
      "Epoch 48/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.1030 - val_loss: 0.1065\n",
      "Epoch 49/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1022 - val_loss: 0.1060\n",
      "Epoch 50/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.1015 - val_loss: 0.1062\n",
      "Epoch 51/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1011 - val_loss: 0.1046\n",
      "Epoch 52/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1004 - val_loss: 0.1050\n",
      "Epoch 53/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.1000 - val_loss: 0.1035\n",
      "Epoch 54/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0991 - val_loss: 0.1039\n",
      "Epoch 55/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0988 - val_loss: 0.1028\n",
      "Epoch 56/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0981 - val_loss: 0.1018\n",
      "Epoch 57/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0974 - val_loss: 0.1017\n",
      "Epoch 58/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0969 - val_loss: 0.1012\n",
      "Epoch 59/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.0966 - val_loss: 0.1006\n",
      "Epoch 60/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0958 - val_loss: 0.1003\n",
      "Epoch 61/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0953 - val_loss: 0.0999\n",
      "Epoch 62/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0948 - val_loss: 0.0994\n",
      "Epoch 63/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0943 - val_loss: 0.0983\n",
      "Epoch 64/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0939 - val_loss: 0.0985\n",
      "Epoch 65/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.0934 - val_loss: 0.0977\n",
      "Epoch 66/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.0931 - val_loss: 0.0981\n",
      "Epoch 67/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0926 - val_loss: 0.0980\n",
      "Epoch 68/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0923 - val_loss: 0.0973\n",
      "Epoch 69/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.0920 - val_loss: 0.0967\n",
      "Epoch 70/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0916 - val_loss: 0.0962\n",
      "Epoch 71/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0911 - val_loss: 0.0963\n",
      "Epoch 72/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0909 - val_loss: 0.0958\n",
      "Epoch 73/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0905 - val_loss: 0.0957\n",
      "Epoch 74/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0904 - val_loss: 0.0955\n",
      "Epoch 75/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0899 - val_loss: 0.0946\n",
      "Epoch 76/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0895 - val_loss: 0.0948\n",
      "Epoch 77/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0893 - val_loss: 0.0947\n",
      "Epoch 78/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0891 - val_loss: 0.0943\n",
      "Epoch 79/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0888 - val_loss: 0.0939\n",
      "Epoch 80/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0885 - val_loss: 0.0940\n",
      "Epoch 81/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0883 - val_loss: 0.0941\n",
      "Epoch 82/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0881 - val_loss: 0.0936\n",
      "Epoch 83/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.0878 - val_loss: 0.0935\n",
      "Epoch 84/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0875 - val_loss: 0.0931\n",
      "Epoch 85/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0874 - val_loss: 0.0928\n",
      "Epoch 86/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0870 - val_loss: 0.0934\n",
      "Epoch 87/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0870 - val_loss: 0.0929\n",
      "Epoch 88/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.0865 - val_loss: 0.0929\n",
      "Epoch 89/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0865 - val_loss: 0.0920\n",
      "Epoch 90/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0861 - val_loss: 0.0939\n",
      "Epoch 91/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0864 - val_loss: 0.0918\n",
      "Epoch 92/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0857 - val_loss: 0.0920\n",
      "Epoch 93/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0856 - val_loss: 0.0919\n",
      "Epoch 94/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0853 - val_loss: 0.0916\n",
      "Epoch 95/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0851 - val_loss: 0.0912\n",
      "Epoch 96/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0849 - val_loss: 0.0915\n",
      "Epoch 97/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0847 - val_loss: 0.0909\n",
      "Epoch 98/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0844 - val_loss: 0.0919\n",
      "Epoch 99/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0844 - val_loss: 0.0911\n",
      "Epoch 100/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0842 - val_loss: 0.0905\n",
      "Epoch 101/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0839 - val_loss: 0.0904\n",
      "Epoch 102/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0838 - val_loss: 0.0902\n",
      "Epoch 103/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0836 - val_loss: 0.0904\n",
      "Epoch 104/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0834 - val_loss: 0.0900\n",
      "Epoch 105/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0833 - val_loss: 0.0896\n",
      "Epoch 106/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0830 - val_loss: 0.0897\n",
      "Epoch 107/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0829 - val_loss: 0.0901\n",
      "Epoch 108/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0829 - val_loss: 0.0901\n",
      "Epoch 109/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0827 - val_loss: 0.0909\n",
      "Epoch 110/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.0829 - val_loss: 0.0893\n",
      "Epoch 111/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0823 - val_loss: 0.0890\n",
      "Epoch 112/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0822 - val_loss: 0.0897\n",
      "Epoch 113/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0820 - val_loss: 0.0893\n",
      "Epoch 114/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0819 - val_loss: 0.0891\n",
      "Epoch 115/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0818 - val_loss: 0.0891\n",
      "Epoch 116/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0816 - val_loss: 0.0889\n",
      "Epoch 117/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0815 - val_loss: 0.0887\n",
      "Epoch 118/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0812 - val_loss: 0.0886\n",
      "Epoch 119/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0812 - val_loss: 0.0888\n",
      "Epoch 120/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0811 - val_loss: 0.0890\n",
      "Epoch 121/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0810 - val_loss: 0.0884\n",
      "Epoch 122/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0808 - val_loss: 0.0883\n",
      "Epoch 123/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0807 - val_loss: 0.0879\n",
      "Epoch 124/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0805 - val_loss: 0.0884\n",
      "Epoch 125/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0806 - val_loss: 0.0880\n",
      "Epoch 126/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0803 - val_loss: 0.0885\n",
      "Epoch 127/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.0803 - val_loss: 0.0882\n",
      "Epoch 128/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0801 - val_loss: 0.0877\n",
      "Epoch 129/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0798 - val_loss: 0.0877\n",
      "Epoch 130/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0797 - val_loss: 0.0882\n",
      "Epoch 131/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0797 - val_loss: 0.0878\n",
      "Epoch 132/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0796 - val_loss: 0.0875\n",
      "Epoch 133/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.0794 - val_loss: 0.0876\n",
      "Epoch 134/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0793 - val_loss: 0.0874\n",
      "Epoch 135/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0793 - val_loss: 0.0871\n",
      "Epoch 136/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.0791 - val_loss: 0.0876\n",
      "Epoch 137/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0791 - val_loss: 0.0870\n",
      "Epoch 138/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0789 - val_loss: 0.0868\n",
      "Epoch 139/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0788 - val_loss: 0.0866\n",
      "Epoch 140/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0786 - val_loss: 0.0866\n",
      "Epoch 141/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0784 - val_loss: 0.0873\n",
      "Epoch 142/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0785 - val_loss: 0.0867\n",
      "Epoch 143/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.0785 - val_loss: 0.0876\n",
      "Epoch 144/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0785 - val_loss: 0.0866\n",
      "Epoch 145/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0781 - val_loss: 0.0866\n",
      "Epoch 146/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.0780 - val_loss: 0.0865\n",
      "Epoch 147/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.0778 - val_loss: 0.0871\n",
      "Epoch 148/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0779 - val_loss: 0.0868\n",
      "Epoch 149/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0778 - val_loss: 0.0864\n",
      "Epoch 150/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0777 - val_loss: 0.0865\n",
      "Epoch 151/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0776 - val_loss: 0.0863\n",
      "Epoch 152/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0775 - val_loss: 0.0865\n",
      "Epoch 153/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.0775 - val_loss: 0.0861\n",
      "Epoch 154/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0773 - val_loss: 0.0866\n",
      "Epoch 155/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0773 - val_loss: 0.0861\n",
      "Epoch 156/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0771 - val_loss: 0.0860\n",
      "Epoch 157/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0770 - val_loss: 0.0859\n",
      "Epoch 158/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0769 - val_loss: 0.0857\n",
      "Epoch 159/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0768 - val_loss: 0.0858\n",
      "Epoch 160/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0768 - val_loss: 0.0856\n",
      "Epoch 161/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0767 - val_loss: 0.0861\n",
      "Epoch 162/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0767 - val_loss: 0.0857\n",
      "Epoch 163/250\n",
      "10000/10000 [==============================] - 4s 450us/sample - loss: 0.0766 - val_loss: 0.0855\n",
      "Epoch 164/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0765 - val_loss: 0.0857\n",
      "Epoch 165/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0763 - val_loss: 0.0856\n",
      "Epoch 166/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0763 - val_loss: 0.0855\n",
      "Epoch 167/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0762 - val_loss: 0.0857\n",
      "Epoch 168/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0762 - val_loss: 0.0855\n",
      "Epoch 169/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0761 - val_loss: 0.0854\n",
      "Epoch 170/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0761 - val_loss: 0.0855\n",
      "Epoch 171/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0760 - val_loss: 0.0853\n",
      "Epoch 172/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0761 - val_loss: 0.0855\n",
      "Epoch 173/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0758 - val_loss: 0.0859\n",
      "Epoch 174/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0758 - val_loss: 0.0850\n",
      "Epoch 175/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0757 - val_loss: 0.0851\n",
      "Epoch 176/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.0755 - val_loss: 0.0855\n",
      "Epoch 177/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0755 - val_loss: 0.0849\n",
      "Epoch 178/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0753 - val_loss: 0.0848\n",
      "Epoch 179/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0753 - val_loss: 0.0857\n",
      "Epoch 180/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0755 - val_loss: 0.0850\n",
      "Epoch 181/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0753 - val_loss: 0.0851\n",
      "Epoch 182/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0752 - val_loss: 0.0847\n",
      "Epoch 183/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0751 - val_loss: 0.0850\n",
      "Epoch 184/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0750 - val_loss: 0.0848\n",
      "Epoch 185/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0750 - val_loss: 0.0848\n",
      "Epoch 186/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0750 - val_loss: 0.0852\n",
      "Epoch 187/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0749 - val_loss: 0.0846\n",
      "Epoch 188/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0747 - val_loss: 0.0847\n",
      "Epoch 189/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0747 - val_loss: 0.0850\n",
      "Epoch 190/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0747 - val_loss: 0.0846\n",
      "Epoch 191/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.0746 - val_loss: 0.0847\n",
      "Epoch 192/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0745 - val_loss: 0.0846\n",
      "Epoch 193/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.0745 - val_loss: 0.0848\n",
      "Epoch 194/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0744 - val_loss: 0.0847\n",
      "Epoch 195/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0744 - val_loss: 0.0845\n",
      "Epoch 196/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0743 - val_loss: 0.0846\n",
      "Epoch 197/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0743 - val_loss: 0.0854\n",
      "Epoch 198/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0744 - val_loss: 0.0843\n",
      "Epoch 199/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0742 - val_loss: 0.0848\n",
      "Epoch 200/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0743 - val_loss: 0.0846\n",
      "Epoch 201/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0740 - val_loss: 0.0843\n",
      "Epoch 202/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0740 - val_loss: 0.0843\n",
      "Epoch 203/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0739 - val_loss: 0.0843\n",
      "Epoch 204/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0739 - val_loss: 0.0842\n",
      "Epoch 205/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.0738 - val_loss: 0.0843\n",
      "Epoch 206/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0738 - val_loss: 0.0846\n",
      "Epoch 207/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0739 - val_loss: 0.0844\n",
      "Epoch 208/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0738 - val_loss: 0.0842\n",
      "Epoch 209/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0736 - val_loss: 0.0842\n",
      "Epoch 210/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.0736 - val_loss: 0.0843\n",
      "Epoch 211/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0736 - val_loss: 0.0845\n",
      "Epoch 212/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0736 - val_loss: 0.0842\n",
      "Epoch 213/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0735 - val_loss: 0.0843\n",
      "Epoch 214/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0735 - val_loss: 0.0841\n",
      "Epoch 215/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0734 - val_loss: 0.0840\n",
      "Epoch 216/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0732 - val_loss: 0.0840\n",
      "Epoch 217/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0732 - val_loss: 0.0843\n",
      "Epoch 218/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0733 - val_loss: 0.0840\n",
      "Epoch 219/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.0731 - val_loss: 0.0839\n",
      "Epoch 220/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.0731 - val_loss: 0.0844\n",
      "Epoch 221/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0732 - val_loss: 0.0843\n",
      "Epoch 222/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0731 - val_loss: 0.0838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0730 - val_loss: 0.0839\n",
      "Epoch 224/250\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.0730 - val_loss: 0.0840\n",
      "Epoch 225/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0730 - val_loss: 0.0839\n",
      "Epoch 226/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0729 - val_loss: 0.0843\n",
      "Epoch 227/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0729 - val_loss: 0.0838\n",
      "Epoch 228/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0728 - val_loss: 0.0839\n",
      "Epoch 229/250\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.0727 - val_loss: 0.0840\n",
      "Epoch 230/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0728 - val_loss: 0.0842\n",
      "Epoch 231/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0727 - val_loss: 0.0836\n",
      "Epoch 232/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0726 - val_loss: 0.0838\n",
      "Epoch 233/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0726 - val_loss: 0.0839\n",
      "Epoch 234/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0726 - val_loss: 0.0838\n",
      "Epoch 235/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.0725 - val_loss: 0.0838\n",
      "Epoch 236/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0724 - val_loss: 0.0835\n",
      "Epoch 237/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0723 - val_loss: 0.0839\n",
      "Epoch 238/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0725 - val_loss: 0.0841\n",
      "Epoch 239/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0725 - val_loss: 0.0839\n",
      "Epoch 240/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0724 - val_loss: 0.0839\n",
      "Epoch 241/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0724 - val_loss: 0.0838\n",
      "Epoch 242/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0722 - val_loss: 0.0838\n",
      "Epoch 243/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0722 - val_loss: 0.0841\n",
      "Epoch 244/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.0722 - val_loss: 0.0840\n",
      "Epoch 245/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.0722 - val_loss: 0.0838\n",
      "Epoch 246/250\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.0721 - val_loss: 0.0838\n",
      "Epoch 247/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0720 - val_loss: 0.0837\n",
      "Epoch 248/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0720 - val_loss: 0.0834\n",
      "Epoch 249/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0720 - val_loss: 0.0835\n",
      "Epoch 250/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0719 - val_loss: 0.0834\n"
     ]
    }
   ],
   "source": [
    "result_sa =  autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, \n",
    "                shuffle=True, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoised AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.1\n",
    "noise = noise_level + np.random.rand(X_train.shape[0],X_train.shape[1])\n",
    "X_train_noise = X_train + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_hidden_1 = 484\n",
    "n_hidden_2 = 256\n",
    "n_hidden_3 = 81\n",
    "n_output = n_input\n",
    "\n",
    "##input\n",
    "input_mg = tf.keras.layers.Input(shape=(n_input,))\n",
    "##hideen layer or coding layer\n",
    "encoded_1 = tf.keras.layers.Dense(n_hidden_1, activation='relu')(input_mg)\n",
    "encoded_2 = tf.keras.layers.Dense(n_hidden_2, activation='relu')(encoded_1)\n",
    "encoded_3 = tf.keras.layers.Dense(n_hidden_3, activation='relu')(encoded_2)\n",
    "\n",
    "##output layer\n",
    "decoded_2 = tf.keras.layers.Dense(n_hidden_2, activation='sigmoid')(encoded_3)\n",
    "decoded_1 = tf.keras.layers.Dense(n_hidden_1, activation='sigmoid')(decoded_2)\n",
    "output_mg = tf.keras.layers.Dense(n_output, activation='sigmoid')(decoded_1)\n",
    "##model\n",
    "autoencoder = tf.keras.models.Model(inputs=input_mg, outputs =output_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 50000 samples\n",
      "Epoch 1/250\n",
      "10000/10000 [==============================] - 5s 514us/sample - loss: 0.3189 - val_loss: 0.2659\n",
      "Epoch 2/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.2624 - val_loss: 0.2642\n",
      "Epoch 3/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2619 - val_loss: 0.2642\n",
      "Epoch 4/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.2620 - val_loss: 0.2640\n",
      "Epoch 5/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.2619 - val_loss: 0.2637\n",
      "Epoch 6/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.2611 - val_loss: 0.2629\n",
      "Epoch 7/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.2562 - val_loss: 0.2575\n",
      "Epoch 8/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.2452 - val_loss: 0.2641\n",
      "Epoch 9/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.2415 - val_loss: 0.2697\n",
      "Epoch 10/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.2390 - val_loss: 0.2635\n",
      "Epoch 11/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.2362 - val_loss: 0.2527\n",
      "Epoch 12/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.2300 - val_loss: 0.2507\n",
      "Epoch 13/250\n",
      "10000/10000 [==============================] - 4s 439us/sample - loss: 0.2231 - val_loss: 0.2389\n",
      "Epoch 14/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.2170 - val_loss: 0.2362\n",
      "Epoch 15/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.2122 - val_loss: 0.2329\n",
      "Epoch 16/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.2075 - val_loss: 0.2218\n",
      "Epoch 17/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.2034 - val_loss: 0.2166\n",
      "Epoch 18/250\n",
      "10000/10000 [==============================] - 4s 418us/sample - loss: 0.1990 - val_loss: 0.2167\n",
      "Epoch 19/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.1953 - val_loss: 0.2172\n",
      "Epoch 20/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.1904 - val_loss: 0.2200\n",
      "Epoch 21/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.1850 - val_loss: 0.2201\n",
      "Epoch 22/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.1807 - val_loss: 0.2189\n",
      "Epoch 23/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.1770 - val_loss: 0.2165\n",
      "Epoch 24/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.1737 - val_loss: 0.2171\n",
      "Epoch 25/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.1710 - val_loss: 0.2127\n",
      "Epoch 26/250\n",
      "10000/10000 [==============================] - 5s 477us/sample - loss: 0.1686 - val_loss: 0.2152\n",
      "Epoch 27/250\n",
      "10000/10000 [==============================] - 5s 496us/sample - loss: 0.1668 - val_loss: 0.2141\n",
      "Epoch 28/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.1650 - val_loss: 0.2168\n",
      "Epoch 29/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.1630 - val_loss: 0.2193\n",
      "Epoch 30/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.1610 - val_loss: 0.2217\n",
      "Epoch 31/250\n",
      "10000/10000 [==============================] - 4s 404us/sample - loss: 0.1588 - val_loss: 0.2213\n",
      "Epoch 32/250\n",
      "10000/10000 [==============================] - 4s 405us/sample - loss: 0.1565 - val_loss: 0.2319\n",
      "Epoch 33/250\n",
      "10000/10000 [==============================] - 4s 410us/sample - loss: 0.1546 - val_loss: 0.2317\n",
      "Epoch 34/250\n",
      "10000/10000 [==============================] - 4s 403us/sample - loss: 0.1530 - val_loss: 0.2300\n",
      "Epoch 35/250\n",
      "10000/10000 [==============================] - 4s 406us/sample - loss: 0.1515 - val_loss: 0.2296\n",
      "Epoch 36/250\n",
      "10000/10000 [==============================] - 4s 406us/sample - loss: 0.1503 - val_loss: 0.2288\n",
      "Epoch 37/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1485 - val_loss: 0.2399\n",
      "Epoch 38/250\n",
      "10000/10000 [==============================] - 4s 400us/sample - loss: 0.1478 - val_loss: 0.2348\n",
      "Epoch 39/250\n",
      "10000/10000 [==============================] - 4s 410us/sample - loss: 0.1462 - val_loss: 0.2345\n",
      "Epoch 40/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1451 - val_loss: 0.2294\n",
      "Epoch 41/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1439 - val_loss: 0.2338\n",
      "Epoch 42/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.1429 - val_loss: 0.2284\n",
      "Epoch 43/250\n",
      "10000/10000 [==============================] - 4s 401us/sample - loss: 0.1414 - val_loss: 0.2305\n",
      "Epoch 44/250\n",
      "10000/10000 [==============================] - 4s 401us/sample - loss: 0.1403 - val_loss: 0.2380\n",
      "Epoch 45/250\n",
      "10000/10000 [==============================] - 4s 404us/sample - loss: 0.1391 - val_loss: 0.2328\n",
      "Epoch 46/250\n",
      "10000/10000 [==============================] - 4s 407us/sample - loss: 0.1381 - val_loss: 0.2330\n",
      "Epoch 47/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1367 - val_loss: 0.2320\n",
      "Epoch 48/250\n",
      "10000/10000 [==============================] - 4s 400us/sample - loss: 0.1356 - val_loss: 0.2301\n",
      "Epoch 49/250\n",
      "10000/10000 [==============================] - 4s 412us/sample - loss: 0.1347 - val_loss: 0.2292\n",
      "Epoch 50/250\n",
      "10000/10000 [==============================] - 4s 406us/sample - loss: 0.1335 - val_loss: 0.2318\n",
      "Epoch 51/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1324 - val_loss: 0.2351\n",
      "Epoch 52/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1315 - val_loss: 0.2295\n",
      "Epoch 53/250\n",
      "10000/10000 [==============================] - 4s 407us/sample - loss: 0.1304 - val_loss: 0.2326\n",
      "Epoch 54/250\n",
      "10000/10000 [==============================] - 4s 405us/sample - loss: 0.1294 - val_loss: 0.2334\n",
      "Epoch 55/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1287 - val_loss: 0.2339\n",
      "Epoch 56/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1279 - val_loss: 0.2362\n",
      "Epoch 57/250\n",
      "10000/10000 [==============================] - 4s 406us/sample - loss: 0.1269 - val_loss: 0.2325\n",
      "Epoch 58/250\n",
      "10000/10000 [==============================] - 4s 398us/sample - loss: 0.1262 - val_loss: 0.2421\n",
      "Epoch 59/250\n",
      "10000/10000 [==============================] - 4s 406us/sample - loss: 0.1255 - val_loss: 0.2315\n",
      "Epoch 60/250\n",
      "10000/10000 [==============================] - 4s 399us/sample - loss: 0.1244 - val_loss: 0.2363\n",
      "Epoch 61/250\n",
      "10000/10000 [==============================] - 4s 400us/sample - loss: 0.1242 - val_loss: 0.2377\n",
      "Epoch 62/250\n",
      "10000/10000 [==============================] - 4s 400us/sample - loss: 0.1231 - val_loss: 0.2345\n",
      "Epoch 63/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1223 - val_loss: 0.2369\n",
      "Epoch 64/250\n",
      "10000/10000 [==============================] - 4s 405us/sample - loss: 0.1218 - val_loss: 0.2383\n",
      "Epoch 65/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1211 - val_loss: 0.2406\n",
      "Epoch 66/250\n",
      "10000/10000 [==============================] - 4s 399us/sample - loss: 0.1202 - val_loss: 0.2391\n",
      "Epoch 67/250\n",
      "10000/10000 [==============================] - 4s 403us/sample - loss: 0.1198 - val_loss: 0.2386\n",
      "Epoch 68/250\n",
      "10000/10000 [==============================] - 4s 404us/sample - loss: 0.1189 - val_loss: 0.2423\n",
      "Epoch 69/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1182 - val_loss: 0.2476\n",
      "Epoch 70/250\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.1181 - val_loss: 0.2396\n",
      "Epoch 71/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1171 - val_loss: 0.2453\n",
      "Epoch 72/250\n",
      "10000/10000 [==============================] - 4s 410us/sample - loss: 0.1164 - val_loss: 0.2490\n",
      "Epoch 73/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.1166 - val_loss: 0.2455\n",
      "Epoch 74/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.1156 - val_loss: 0.2474\n",
      "Epoch 75/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.1150 - val_loss: 0.2419\n",
      "Epoch 76/250\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 0.1144 - val_loss: 0.2410\n",
      "Epoch 77/250\n",
      "10000/10000 [==============================] - 5s 476us/sample - loss: 0.1141 - val_loss: 0.2467\n",
      "Epoch 78/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.1135 - val_loss: 0.2468\n",
      "Epoch 79/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.1130 - val_loss: 0.2452\n",
      "Epoch 80/250\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 0.1124 - val_loss: 0.2453\n",
      "Epoch 81/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.1120 - val_loss: 0.2462\n",
      "Epoch 82/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.1118 - val_loss: 0.2481\n",
      "Epoch 83/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.1112 - val_loss: 0.2452\n",
      "Epoch 84/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.1107 - val_loss: 0.2473\n",
      "Epoch 85/250\n",
      "10000/10000 [==============================] - 4s 407us/sample - loss: 0.1101 - val_loss: 0.2459\n",
      "Epoch 86/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.1097 - val_loss: 0.2512\n",
      "Epoch 87/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.1097 - val_loss: 0.2510\n",
      "Epoch 88/250\n",
      "10000/10000 [==============================] - 5s 476us/sample - loss: 0.1090 - val_loss: 0.2442\n",
      "Epoch 89/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.1086 - val_loss: 0.2441\n",
      "Epoch 90/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.1080 - val_loss: 0.2462\n",
      "Epoch 91/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1079 - val_loss: 0.2450\n",
      "Epoch 92/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.1075 - val_loss: 0.2425\n",
      "Epoch 93/250\n",
      "10000/10000 [==============================] - 4s 402us/sample - loss: 0.1073 - val_loss: 0.2453\n",
      "Epoch 94/250\n",
      "10000/10000 [==============================] - 4s 403us/sample - loss: 0.1069 - val_loss: 0.2476\n",
      "Epoch 95/250\n",
      "10000/10000 [==============================] - 4s 409us/sample - loss: 0.1062 - val_loss: 0.2498\n",
      "Epoch 96/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.1059 - val_loss: 0.2520\n",
      "Epoch 97/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.1055 - val_loss: 0.2471\n",
      "Epoch 98/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.1051 - val_loss: 0.2475\n",
      "Epoch 99/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.1049 - val_loss: 0.2476\n",
      "Epoch 100/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.1045 - val_loss: 0.2506\n",
      "Epoch 101/250\n",
      "10000/10000 [==============================] - 5s 474us/sample - loss: 0.1045 - val_loss: 0.2424\n",
      "Epoch 102/250\n",
      "10000/10000 [==============================] - 5s 461us/sample - loss: 0.1040 - val_loss: 0.2499\n",
      "Epoch 103/250\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 0.1034 - val_loss: 0.2412\n",
      "Epoch 104/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.1034 - val_loss: 0.2489\n",
      "Epoch 105/250\n",
      "10000/10000 [==============================] - 4s 408us/sample - loss: 0.1028 - val_loss: 0.2492\n",
      "Epoch 106/250\n",
      "10000/10000 [==============================] - 4s 409us/sample - loss: 0.1027 - val_loss: 0.2499\n",
      "Epoch 107/250\n",
      "10000/10000 [==============================] - 4s 434us/sample - loss: 0.1022 - val_loss: 0.2490\n",
      "Epoch 108/250\n",
      "10000/10000 [==============================] - 5s 501us/sample - loss: 0.1021 - val_loss: 0.2473\n",
      "Epoch 109/250\n",
      "10000/10000 [==============================] - 5s 480us/sample - loss: 0.1020 - val_loss: 0.2512\n",
      "Epoch 110/250\n",
      "10000/10000 [==============================] - 5s 494us/sample - loss: 0.1018 - val_loss: 0.2567\n",
      "Epoch 111/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.1014 - val_loss: 0.2535\n",
      "Epoch 112/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.1006 - val_loss: 0.2557\n",
      "Epoch 113/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.1009 - val_loss: 0.2553\n",
      "Epoch 114/250\n",
      "10000/10000 [==============================] - 5s 481us/sample - loss: 0.1005 - val_loss: 0.2506\n",
      "Epoch 115/250\n",
      "10000/10000 [==============================] - 4s 405us/sample - loss: 0.1001 - val_loss: 0.2506\n",
      "Epoch 116/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.0997 - val_loss: 0.2558\n",
      "Epoch 117/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0998 - val_loss: 0.2571\n",
      "Epoch 118/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.0995 - val_loss: 0.2565\n",
      "Epoch 119/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0992 - val_loss: 0.2473\n",
      "Epoch 120/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0992 - val_loss: 0.2490\n",
      "Epoch 121/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0985 - val_loss: 0.2497\n",
      "Epoch 122/250\n",
      "10000/10000 [==============================] - 5s 450us/sample - loss: 0.0984 - val_loss: 0.2507\n",
      "Epoch 123/250\n",
      "10000/10000 [==============================] - 5s 516us/sample - loss: 0.0981 - val_loss: 0.2522\n",
      "Epoch 124/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.0979 - val_loss: 0.2485\n",
      "Epoch 125/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.0979 - val_loss: 0.2519\n",
      "Epoch 126/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.0973 - val_loss: 0.2525\n",
      "Epoch 127/250\n",
      "10000/10000 [==============================] - 5s 472us/sample - loss: 0.0973 - val_loss: 0.2509\n",
      "Epoch 128/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.0972 - val_loss: 0.2509\n",
      "Epoch 129/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0968 - val_loss: 0.2508\n",
      "Epoch 130/250\n",
      "10000/10000 [==============================] - 4s 417us/sample - loss: 0.0965 - val_loss: 0.2549\n",
      "Epoch 131/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0965 - val_loss: 0.2502\n",
      "Epoch 132/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.0962 - val_loss: 0.2535\n",
      "Epoch 133/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.0961 - val_loss: 0.2510\n",
      "Epoch 134/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.0959 - val_loss: 0.2498\n",
      "Epoch 135/250\n",
      "10000/10000 [==============================] - 4s 404us/sample - loss: 0.0956 - val_loss: 0.2485\n",
      "Epoch 136/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.0954 - val_loss: 0.2491\n",
      "Epoch 137/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0954 - val_loss: 0.2499\n",
      "Epoch 138/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.0950 - val_loss: 0.2529\n",
      "Epoch 139/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.0948 - val_loss: 0.2553\n",
      "Epoch 140/250\n",
      "10000/10000 [==============================] - 6s 596us/sample - loss: 0.0949 - val_loss: 0.2513\n",
      "Epoch 141/250\n",
      "10000/10000 [==============================] - 7s 675us/sample - loss: 0.0946 - val_loss: 0.2535\n",
      "Epoch 142/250\n",
      "10000/10000 [==============================] - 6s 611us/sample - loss: 0.0943 - val_loss: 0.2530\n",
      "Epoch 143/250\n",
      "10000/10000 [==============================] - 5s 540us/sample - loss: 0.0939 - val_loss: 0.2488\n",
      "Epoch 144/250\n",
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.0942 - val_loss: 0.2512\n",
      "Epoch 145/250\n",
      "10000/10000 [==============================] - 5s 477us/sample - loss: 0.0937 - val_loss: 0.2502\n",
      "Epoch 146/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.0938 - val_loss: 0.2542\n",
      "Epoch 147/250\n",
      "10000/10000 [==============================] - 5s 494us/sample - loss: 0.0934 - val_loss: 0.2502\n",
      "Epoch 148/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.0932 - val_loss: 0.2547\n",
      "Epoch 149/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 472us/sample - loss: 0.0930 - val_loss: 0.2585\n",
      "Epoch 150/250\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 0.0928 - val_loss: 0.2532\n",
      "Epoch 151/250\n",
      "10000/10000 [==============================] - 5s 467us/sample - loss: 0.0926 - val_loss: 0.2531\n",
      "Epoch 152/250\n",
      "10000/10000 [==============================] - 5s 456us/sample - loss: 0.0926 - val_loss: 0.2509\n",
      "Epoch 153/250\n",
      "10000/10000 [==============================] - 5s 463us/sample - loss: 0.0924 - val_loss: 0.2492\n",
      "Epoch 154/250\n",
      "10000/10000 [==============================] - 4s 416us/sample - loss: 0.0922 - val_loss: 0.2593\n",
      "Epoch 155/250\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 0.0922 - val_loss: 0.2569\n",
      "Epoch 156/250\n",
      "10000/10000 [==============================] - 5s 539us/sample - loss: 0.0918 - val_loss: 0.2552\n",
      "Epoch 157/250\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 0.0918 - val_loss: 0.2578\n",
      "Epoch 158/250\n",
      "10000/10000 [==============================] - 5s 462us/sample - loss: 0.0916 - val_loss: 0.2567\n",
      "Epoch 159/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0913 - val_loss: 0.2523\n",
      "Epoch 160/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.0913 - val_loss: 0.2514\n",
      "Epoch 161/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.0915 - val_loss: 0.2526\n",
      "Epoch 162/250\n",
      "10000/10000 [==============================] - 5s 487us/sample - loss: 0.0911 - val_loss: 0.2533\n",
      "Epoch 163/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.0910 - val_loss: 0.2525\n",
      "Epoch 164/250\n",
      "10000/10000 [==============================] - 5s 500us/sample - loss: 0.0908 - val_loss: 0.2551\n",
      "Epoch 165/250\n",
      "10000/10000 [==============================] - 5s 456us/sample - loss: 0.0906 - val_loss: 0.2522\n",
      "Epoch 166/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0906 - val_loss: 0.2494\n",
      "Epoch 167/250\n",
      "10000/10000 [==============================] - 5s 515us/sample - loss: 0.0904 - val_loss: 0.2602\n",
      "Epoch 168/250\n",
      "10000/10000 [==============================] - 5s 524us/sample - loss: 0.0903 - val_loss: 0.2570\n",
      "Epoch 169/250\n",
      "10000/10000 [==============================] - 5s 509us/sample - loss: 0.0900 - val_loss: 0.2552\n",
      "Epoch 170/250\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 0.0899 - val_loss: 0.2526\n",
      "Epoch 171/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.0897 - val_loss: 0.2591\n",
      "Epoch 172/250\n",
      "10000/10000 [==============================] - 5s 543us/sample - loss: 0.0897 - val_loss: 0.2546\n",
      "Epoch 173/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.0895 - val_loss: 0.2557\n",
      "Epoch 174/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.0894 - val_loss: 0.2538\n",
      "Epoch 175/250\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 0.0893 - val_loss: 0.2590\n",
      "Epoch 176/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.0891 - val_loss: 0.2580\n",
      "Epoch 177/250\n",
      "10000/10000 [==============================] - 5s 530us/sample - loss: 0.0889 - val_loss: 0.2565\n",
      "Epoch 178/250\n",
      "10000/10000 [==============================] - 5s 482us/sample - loss: 0.0887 - val_loss: 0.2543\n",
      "Epoch 179/250\n",
      "10000/10000 [==============================] - 5s 516us/sample - loss: 0.0885 - val_loss: 0.2588\n",
      "Epoch 180/250\n",
      "10000/10000 [==============================] - 5s 540us/sample - loss: 0.0885 - val_loss: 0.2560\n",
      "Epoch 181/250\n",
      "10000/10000 [==============================] - 4s 449us/sample - loss: 0.0884 - val_loss: 0.2607\n",
      "Epoch 182/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.0885 - val_loss: 0.2572\n",
      "Epoch 183/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.0885 - val_loss: 0.2522\n",
      "Epoch 184/250\n",
      "10000/10000 [==============================] - 5s 454us/sample - loss: 0.0881 - val_loss: 0.2569\n",
      "Epoch 185/250\n",
      "10000/10000 [==============================] - 4s 442us/sample - loss: 0.0884 - val_loss: 0.2626\n",
      "Epoch 186/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.0882 - val_loss: 0.2605\n",
      "Epoch 187/250\n",
      "10000/10000 [==============================] - 5s 479us/sample - loss: 0.0879 - val_loss: 0.2598\n",
      "Epoch 188/250\n",
      "10000/10000 [==============================] - 5s 514us/sample - loss: 0.0876 - val_loss: 0.2603\n",
      "Epoch 189/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.0876 - val_loss: 0.2594\n",
      "Epoch 190/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.0874 - val_loss: 0.2600\n",
      "Epoch 191/250\n",
      "10000/10000 [==============================] - 5s 458us/sample - loss: 0.0871 - val_loss: 0.2629\n",
      "Epoch 192/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.0871 - val_loss: 0.2573\n",
      "Epoch 193/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.0870 - val_loss: 0.2586\n",
      "Epoch 194/250\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 0.0869 - val_loss: 0.2594\n",
      "Epoch 195/250\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 0.0869 - val_loss: 0.2566\n",
      "Epoch 196/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.0868 - val_loss: 0.2576\n",
      "Epoch 197/250\n",
      "10000/10000 [==============================] - 5s 482us/sample - loss: 0.0868 - val_loss: 0.2541\n",
      "Epoch 198/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.0864 - val_loss: 0.2585\n",
      "Epoch 199/250\n",
      "10000/10000 [==============================] - 5s 501us/sample - loss: 0.0866 - val_loss: 0.2553\n",
      "Epoch 200/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0867 - val_loss: 0.2625\n",
      "Epoch 201/250\n",
      "10000/10000 [==============================] - 5s 490us/sample - loss: 0.0865 - val_loss: 0.2571\n",
      "Epoch 202/250\n",
      "10000/10000 [==============================] - 5s 477us/sample - loss: 0.0861 - val_loss: 0.2578\n",
      "Epoch 203/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.0860 - val_loss: 0.2577\n",
      "Epoch 204/250\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 0.0860 - val_loss: 0.2536\n",
      "Epoch 205/250\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 0.0859 - val_loss: 0.2588\n",
      "Epoch 206/250\n",
      "10000/10000 [==============================] - 5s 530us/sample - loss: 0.0860 - val_loss: 0.2600\n",
      "Epoch 207/250\n",
      "10000/10000 [==============================] - 5s 463us/sample - loss: 0.0858 - val_loss: 0.2622\n",
      "Epoch 208/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.0855 - val_loss: 0.2575\n",
      "Epoch 209/250\n",
      "10000/10000 [==============================] - 5s 483us/sample - loss: 0.0856 - val_loss: 0.2586\n",
      "Epoch 210/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.0853 - val_loss: 0.2571\n",
      "Epoch 211/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.0852 - val_loss: 0.2575\n",
      "Epoch 212/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.0852 - val_loss: 0.2549\n",
      "Epoch 213/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.0850 - val_loss: 0.2575\n",
      "Epoch 214/250\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 0.0850 - val_loss: 0.2555\n",
      "Epoch 215/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.0849 - val_loss: 0.2590\n",
      "Epoch 216/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0848 - val_loss: 0.2586\n",
      "Epoch 217/250\n",
      "10000/10000 [==============================] - 4s 447us/sample - loss: 0.0848 - val_loss: 0.2625\n",
      "Epoch 218/250\n",
      "10000/10000 [==============================] - 6s 592us/sample - loss: 0.0847 - val_loss: 0.2569\n",
      "Epoch 219/250\n",
      "10000/10000 [==============================] - 6s 607us/sample - loss: 0.0847 - val_loss: 0.2638\n",
      "Epoch 220/250\n",
      "10000/10000 [==============================] - 5s 509us/sample - loss: 0.0851 - val_loss: 0.2604\n",
      "Epoch 221/250\n",
      "10000/10000 [==============================] - 5s 486us/sample - loss: 0.0846 - val_loss: 0.2533\n",
      "Epoch 222/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.0846 - val_loss: 0.2559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.0844 - val_loss: 0.2581\n",
      "Epoch 224/250\n",
      "10000/10000 [==============================] - 4s 434us/sample - loss: 0.0843 - val_loss: 0.2622\n",
      "Epoch 225/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.0840 - val_loss: 0.2616\n",
      "Epoch 226/250\n",
      "10000/10000 [==============================] - 4s 449us/sample - loss: 0.0841 - val_loss: 0.2580\n",
      "Epoch 227/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.0839 - val_loss: 0.2619\n",
      "Epoch 228/250\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 0.0838 - val_loss: 0.2639\n",
      "Epoch 229/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0838 - val_loss: 0.2608\n",
      "Epoch 230/250\n",
      "10000/10000 [==============================] - 4s 449us/sample - loss: 0.0835 - val_loss: 0.2646\n",
      "Epoch 231/250\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 0.0837 - val_loss: 0.2584\n",
      "Epoch 232/250\n",
      "10000/10000 [==============================] - 5s 473us/sample - loss: 0.0836 - val_loss: 0.2650\n",
      "Epoch 233/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0838 - val_loss: 0.2621\n",
      "Epoch 234/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.0835 - val_loss: 0.2632\n",
      "Epoch 235/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.0832 - val_loss: 0.2572\n",
      "Epoch 236/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.0834 - val_loss: 0.2613\n",
      "Epoch 237/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.0833 - val_loss: 0.2608\n",
      "Epoch 238/250\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 0.0833 - val_loss: 0.2602\n",
      "Epoch 239/250\n",
      "10000/10000 [==============================] - 5s 479us/sample - loss: 0.0834 - val_loss: 0.2622\n",
      "Epoch 240/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0830 - val_loss: 0.2637\n",
      "Epoch 241/250\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 0.0829 - val_loss: 0.2590\n",
      "Epoch 242/250\n",
      "10000/10000 [==============================] - 4s 419us/sample - loss: 0.0828 - val_loss: 0.2675\n",
      "Epoch 243/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0826 - val_loss: 0.2584\n",
      "Epoch 244/250\n",
      "10000/10000 [==============================] - 4s 405us/sample - loss: 0.0828 - val_loss: 0.2604\n",
      "Epoch 245/250\n",
      "10000/10000 [==============================] - 4s 413us/sample - loss: 0.0826 - val_loss: 0.2587\n",
      "Epoch 246/250\n",
      "10000/10000 [==============================] - 4s 415us/sample - loss: 0.0826 - val_loss: 0.2595\n",
      "Epoch 247/250\n",
      "10000/10000 [==============================] - 4s 409us/sample - loss: 0.0824 - val_loss: 0.2598\n",
      "Epoch 248/250\n",
      "10000/10000 [==============================] - 4s 414us/sample - loss: 0.0828 - val_loss: 0.2597\n",
      "Epoch 249/250\n",
      "10000/10000 [==============================] - 4s 408us/sample - loss: 0.0824 - val_loss: 0.2613\n",
      "Epoch 250/250\n",
      "10000/10000 [==============================] - 4s 411us/sample - loss: 0.0825 - val_loss: 0.2628\n"
     ]
    }
   ],
   "source": [
    "result_da = autoencoder.fit(X_train_noise, X_train, epochs=epochs, batch_size=batch_size, \n",
    "                shuffle=True, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_hidden_1 = 484\n",
    "n_hidden_2 = 256\n",
    "n_hidden_3 = 81\n",
    "n_output = n_input\n",
    "\n",
    "##input\n",
    "input_mg = tf.keras.layers.Input(shape=(n_input,))\n",
    "##hideen layer or coding layer\n",
    "encoded_1 = tf.keras.layers.Dense(n_hidden_1, activation='relu', activity_regularizer=tf.keras.regularizers.l1())(input_mg)\n",
    "encoded_2 = tf.keras.layers.Dense(n_hidden_2, activation='relu', activity_regularizer=tf.keras.regularizers.l1())(encoded_1)\n",
    "encoded_3 = tf.keras.layers.Dense(n_hidden_3, activation='relu', activity_regularizer=tf.keras.regularizers.l1())(encoded_2)\n",
    "\n",
    "##output layer\n",
    "decoded_2 = tf.keras.layers.Dense(n_hidden_2, activation='sigmoid', activity_regularizer=tf.keras.regularizers.l1())(encoded_3)\n",
    "decoded_1 = tf.keras.layers.Dense(n_hidden_1, activation='sigmoid', activity_regularizer=tf.keras.regularizers.l1())(decoded_2)\n",
    "output_mg = tf.keras.layers.Dense(n_output, activation='sigmoid')(decoded_1)\n",
    "##model\n",
    "autoencoder = tf.keras.models.Model(inputs=input_mg, outputs =output_mg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 50000 samples\n",
      "Epoch 1/250\n",
      "10000/10000 [==============================] - 4s 447us/sample - loss: 0.3854 - val_loss: 0.4012\n",
      "Epoch 2/250\n",
      "10000/10000 [==============================] - 5s 504us/sample - loss: 0.3837 - val_loss: 0.3995\n",
      "Epoch 3/250\n",
      "10000/10000 [==============================] - 5s 492us/sample - loss: 0.3820 - val_loss: 0.3979\n",
      "Epoch 4/250\n",
      "10000/10000 [==============================] - 5s 455us/sample - loss: 0.3804 - val_loss: 0.3962\n",
      "Epoch 5/250\n",
      "10000/10000 [==============================] - 5s 524us/sample - loss: 0.3788 - val_loss: 0.3946\n",
      "Epoch 6/250\n",
      "10000/10000 [==============================] - 5s 494us/sample - loss: 0.3772 - val_loss: 0.3931\n",
      "Epoch 7/250\n",
      "10000/10000 [==============================] - 5s 535us/sample - loss: 0.3756 - val_loss: 0.3915\n",
      "Epoch 8/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.3741 - val_loss: 0.3900\n",
      "Epoch 9/250\n",
      "10000/10000 [==============================] - 4s 450us/sample - loss: 0.3725 - val_loss: 0.3885\n",
      "Epoch 10/250\n",
      "10000/10000 [==============================] - 5s 455us/sample - loss: 0.3711 - val_loss: 0.3870\n",
      "Epoch 11/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.3696 - val_loss: 0.3856\n",
      "Epoch 12/250\n",
      "10000/10000 [==============================] - 5s 458us/sample - loss: 0.3682 - val_loss: 0.3841\n",
      "Epoch 13/250\n",
      "10000/10000 [==============================] - 5s 455us/sample - loss: 0.3667 - val_loss: 0.3827\n",
      "Epoch 14/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.3653 - val_loss: 0.3813\n",
      "Epoch 15/250\n",
      "10000/10000 [==============================] - 5s 450us/sample - loss: 0.3640 - val_loss: 0.3800\n",
      "Epoch 16/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.3626 - val_loss: 0.3786\n",
      "Epoch 17/250\n",
      "10000/10000 [==============================] - 5s 514us/sample - loss: 0.3613 - val_loss: 0.3773\n",
      "Epoch 18/250\n",
      "10000/10000 [==============================] - 5s 497us/sample - loss: 0.3599 - val_loss: 0.3760\n",
      "Epoch 19/250\n",
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.3586 - val_loss: 0.3747\n",
      "Epoch 20/250\n",
      "10000/10000 [==============================] - 5s 473us/sample - loss: 0.3573 - val_loss: 0.3734\n",
      "Epoch 21/250\n",
      "10000/10000 [==============================] - 5s 461us/sample - loss: 0.3561 - val_loss: 0.3721\n",
      "Epoch 22/250\n",
      "10000/10000 [==============================] - 6s 603us/sample - loss: 0.3548 - val_loss: 0.3709\n",
      "Epoch 23/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.3536 - val_loss: 0.3697\n",
      "Epoch 24/250\n",
      "10000/10000 [==============================] - 5s 455us/sample - loss: 0.3524 - val_loss: 0.3685\n",
      "Epoch 25/250\n",
      "10000/10000 [==============================] - 5s 474us/sample - loss: 0.3512 - val_loss: 0.3673\n",
      "Epoch 26/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.3500 - val_loss: 0.3661\n",
      "Epoch 27/250\n",
      "10000/10000 [==============================] - 5s 495us/sample - loss: 0.3488 - val_loss: 0.3649\n",
      "Epoch 28/250\n",
      "10000/10000 [==============================] - 5s 458us/sample - loss: 0.3476 - val_loss: 0.3638\n",
      "Epoch 29/250\n",
      "10000/10000 [==============================] - 5s 467us/sample - loss: 0.3465 - val_loss: 0.3626\n",
      "Epoch 30/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.3454 - val_loss: 0.3615\n",
      "Epoch 31/250\n",
      "10000/10000 [==============================] - 5s 467us/sample - loss: 0.3443 - val_loss: 0.3604\n",
      "Epoch 32/250\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 0.3432 - val_loss: 0.3593\n",
      "Epoch 33/250\n",
      "10000/10000 [==============================] - 5s 511us/sample - loss: 0.3421 - val_loss: 0.3582\n",
      "Epoch 34/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.3410 - val_loss: 0.3572\n",
      "Epoch 35/250\n",
      "10000/10000 [==============================] - 5s 485us/sample - loss: 0.3400 - val_loss: 0.3561\n",
      "Epoch 36/250\n",
      "10000/10000 [==============================] - 5s 467us/sample - loss: 0.3389 - val_loss: 0.3551\n",
      "Epoch 37/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.3379 - val_loss: 0.3541\n",
      "Epoch 38/250\n",
      "10000/10000 [==============================] - 5s 479us/sample - loss: 0.3369 - val_loss: 0.3531\n",
      "Epoch 39/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.3359 - val_loss: 0.3521\n",
      "Epoch 40/250\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 0.3349 - val_loss: 0.3511\n",
      "Epoch 41/250\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 0.3339 - val_loss: 0.3501\n",
      "Epoch 42/250\n",
      "10000/10000 [==============================] - 5s 454us/sample - loss: 0.3329 - val_loss: 0.3492\n",
      "Epoch 43/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.3320 - val_loss: 0.3482\n",
      "Epoch 44/250\n",
      "10000/10000 [==============================] - 5s 504us/sample - loss: 0.3311 - val_loss: 0.3473\n",
      "Epoch 45/250\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 0.3301 - val_loss: 0.3464\n",
      "Epoch 46/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.3292 - val_loss: 0.3455\n",
      "Epoch 47/250\n",
      "10000/10000 [==============================] - 5s 452us/sample - loss: 0.3283 - val_loss: 0.3446\n",
      "Epoch 48/250\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 0.3274 - val_loss: 0.3437\n",
      "Epoch 49/250\n",
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.3265 - val_loss: 0.3428\n",
      "Epoch 50/250\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 0.3256 - val_loss: 0.3419\n",
      "Epoch 51/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.3248 - val_loss: 0.3411\n",
      "Epoch 52/250\n",
      "10000/10000 [==============================] - 5s 479us/sample - loss: 0.3239 - val_loss: 0.3402\n",
      "Epoch 53/250\n",
      "10000/10000 [==============================] - 5s 476us/sample - loss: 0.3231 - val_loss: 0.3394\n",
      "Epoch 54/250\n",
      "10000/10000 [==============================] - 5s 523us/sample - loss: 0.3223 - val_loss: 0.3386\n",
      "Epoch 55/250\n",
      "10000/10000 [==============================] - 6s 582us/sample - loss: 0.3214 - val_loss: 0.3377\n",
      "Epoch 56/250\n",
      "10000/10000 [==============================] - 5s 474us/sample - loss: 0.3206 - val_loss: 0.3369\n",
      "Epoch 57/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.3198 - val_loss: 0.3361\n",
      "Epoch 58/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.3190 - val_loss: 0.3353\n",
      "Epoch 59/250\n",
      "10000/10000 [==============================] - 5s 496us/sample - loss: 0.3182 - val_loss: 0.3346\n",
      "Epoch 60/250\n",
      "10000/10000 [==============================] - 5s 493us/sample - loss: 0.3175 - val_loss: 0.3338\n",
      "Epoch 61/250\n",
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.3167 - val_loss: 0.3330\n",
      "Epoch 62/250\n",
      "10000/10000 [==============================] - 4s 447us/sample - loss: 0.3159 - val_loss: 0.3323\n",
      "Epoch 63/250\n",
      "10000/10000 [==============================] - 5s 452us/sample - loss: 0.3152 - val_loss: 0.3315\n",
      "Epoch 64/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.3144 - val_loss: 0.3308\n",
      "Epoch 65/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.3137 - val_loss: 0.3301\n",
      "Epoch 66/250\n",
      "10000/10000 [==============================] - 5s 496us/sample - loss: 0.3130 - val_loss: 0.3294\n",
      "Epoch 67/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.3123 - val_loss: 0.3286\n",
      "Epoch 68/250\n",
      "10000/10000 [==============================] - 5s 524us/sample - loss: 0.3116 - val_loss: 0.3279\n",
      "Epoch 69/250\n",
      "10000/10000 [==============================] - 5s 515us/sample - loss: 0.3109 - val_loss: 0.3273\n",
      "Epoch 70/250\n",
      "10000/10000 [==============================] - 6s 588us/sample - loss: 0.3102 - val_loss: 0.3266\n",
      "Epoch 71/250\n",
      "10000/10000 [==============================] - 4s 446us/sample - loss: 0.3095 - val_loss: 0.3259\n",
      "Epoch 72/250\n",
      "10000/10000 [==============================] - 5s 465us/sample - loss: 0.3088 - val_loss: 0.3252\n",
      "Epoch 73/250\n",
      "10000/10000 [==============================] - 5s 491us/sample - loss: 0.3082 - val_loss: 0.3246\n",
      "Epoch 74/250\n",
      "10000/10000 [==============================] - 5s 473us/sample - loss: 0.3075 - val_loss: 0.3239\n",
      "Epoch 75/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.3069 - val_loss: 0.3233\n",
      "Epoch 76/250\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 0.3062 - val_loss: 0.3226\n",
      "Epoch 77/250\n",
      "10000/10000 [==============================] - 5s 483us/sample - loss: 0.3056 - val_loss: 0.3220\n",
      "Epoch 78/250\n",
      "10000/10000 [==============================] - 5s 493us/sample - loss: 0.3049 - val_loss: 0.3214\n",
      "Epoch 79/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.3043 - val_loss: 0.3207\n",
      "Epoch 80/250\n",
      "10000/10000 [==============================] - 5s 511us/sample - loss: 0.3037 - val_loss: 0.3201\n",
      "Epoch 81/250\n",
      "10000/10000 [==============================] - 5s 500us/sample - loss: 0.3031 - val_loss: 0.3195\n",
      "Epoch 82/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.3025 - val_loss: 0.3189\n",
      "Epoch 83/250\n",
      "10000/10000 [==============================] - 5s 485us/sample - loss: 0.3019 - val_loss: 0.3183\n",
      "Epoch 84/250\n",
      "10000/10000 [==============================] - 5s 494us/sample - loss: 0.3013 - val_loss: 0.3178\n",
      "Epoch 85/250\n",
      "10000/10000 [==============================] - 5s 519us/sample - loss: 0.3007 - val_loss: 0.3172\n",
      "Epoch 86/250\n",
      "10000/10000 [==============================] - 5s 543us/sample - loss: 0.3002 - val_loss: 0.3166\n",
      "Epoch 87/250\n",
      "10000/10000 [==============================] - 5s 510us/sample - loss: 0.2996 - val_loss: 0.3161\n",
      "Epoch 88/250\n",
      "10000/10000 [==============================] - 6s 608us/sample - loss: 0.2990 - val_loss: 0.3155\n",
      "Epoch 89/250\n",
      "10000/10000 [==============================] - 5s 533us/sample - loss: 0.2985 - val_loss: 0.3149\n",
      "Epoch 90/250\n",
      "10000/10000 [==============================] - 5s 514us/sample - loss: 0.2979 - val_loss: 0.3144\n",
      "Epoch 91/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.2974 - val_loss: 0.3139\n",
      "Epoch 92/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.2969 - val_loss: 0.3133\n",
      "Epoch 93/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.2963 - val_loss: 0.3128\n",
      "Epoch 94/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.2958 - val_loss: 0.3123\n",
      "Epoch 95/250\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 0.2953 - val_loss: 0.3118\n",
      "Epoch 96/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.2948 - val_loss: 0.3113\n",
      "Epoch 97/250\n",
      "10000/10000 [==============================] - 5s 506us/sample - loss: 0.2943 - val_loss: 0.3108\n",
      "Epoch 98/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.2938 - val_loss: 0.3103\n",
      "Epoch 99/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.2933 - val_loss: 0.3098\n",
      "Epoch 100/250\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 0.2928 - val_loss: 0.3093\n",
      "Epoch 101/250\n",
      "10000/10000 [==============================] - 5s 463us/sample - loss: 0.2923 - val_loss: 0.3088\n",
      "Epoch 102/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.2919 - val_loss: 0.3084\n",
      "Epoch 103/250\n",
      "10000/10000 [==============================] - 4s 439us/sample - loss: 0.2914 - val_loss: 0.3079\n",
      "Epoch 104/250\n",
      "10000/10000 [==============================] - 5s 472us/sample - loss: 0.2909 - val_loss: 0.3074\n",
      "Epoch 105/250\n",
      "10000/10000 [==============================] - 5s 462us/sample - loss: 0.2905 - val_loss: 0.3070\n",
      "Epoch 106/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.2900 - val_loss: 0.3065\n",
      "Epoch 107/250\n",
      "10000/10000 [==============================] - 5s 452us/sample - loss: 0.2896 - val_loss: 0.3061\n",
      "Epoch 108/250\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 0.2891 - val_loss: 0.3056\n",
      "Epoch 109/250\n",
      "10000/10000 [==============================] - 5s 458us/sample - loss: 0.2887 - val_loss: 0.3052\n",
      "Epoch 110/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.2883 - val_loss: 0.3048\n",
      "Epoch 111/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.2878 - val_loss: 0.3044\n",
      "Epoch 112/250\n",
      "10000/10000 [==============================] - 4s 447us/sample - loss: 0.2874 - val_loss: 0.3039\n",
      "Epoch 113/250\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 0.2870 - val_loss: 0.3035\n",
      "Epoch 114/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2866 - val_loss: 0.3031\n",
      "Epoch 115/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2862 - val_loss: 0.3027\n",
      "Epoch 116/250\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 0.2858 - val_loss: 0.3023\n",
      "Epoch 117/250\n",
      "10000/10000 [==============================] - 5s 484us/sample - loss: 0.2854 - val_loss: 0.3019\n",
      "Epoch 118/250\n",
      "10000/10000 [==============================] - 5s 483us/sample - loss: 0.2850 - val_loss: 0.3016\n",
      "Epoch 119/250\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 0.2846 - val_loss: 0.3012\n",
      "Epoch 120/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.2842 - val_loss: 0.3008\n",
      "Epoch 121/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2839 - val_loss: 0.3004\n",
      "Epoch 122/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.2835 - val_loss: 0.3001\n",
      "Epoch 123/250\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 0.2831 - val_loss: 0.2997\n",
      "Epoch 124/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.2828 - val_loss: 0.2993\n",
      "Epoch 125/250\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 0.2824 - val_loss: 0.2990\n",
      "Epoch 126/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.2821 - val_loss: 0.2986\n",
      "Epoch 127/250\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 0.2817 - val_loss: 0.2983\n",
      "Epoch 128/250\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 0.2814 - val_loss: 0.2980\n",
      "Epoch 129/250\n",
      "10000/10000 [==============================] - 4s 442us/sample - loss: 0.2811 - val_loss: 0.2976\n",
      "Epoch 130/250\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 0.2807 - val_loss: 0.2973\n",
      "Epoch 131/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.2804 - val_loss: 0.2970\n",
      "Epoch 132/250\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 0.2801 - val_loss: 0.2967\n",
      "Epoch 133/250\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 0.2798 - val_loss: 0.2963\n",
      "Epoch 134/250\n",
      "10000/10000 [==============================] - 4s 434us/sample - loss: 0.2794 - val_loss: 0.2960\n",
      "Epoch 135/250\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 0.2791 - val_loss: 0.2957\n",
      "Epoch 136/250\n",
      "10000/10000 [==============================] - 4s 442us/sample - loss: 0.2788 - val_loss: 0.2954\n",
      "Epoch 137/250\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 0.2785 - val_loss: 0.2951\n",
      "Epoch 138/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2782 - val_loss: 0.2948\n",
      "Epoch 139/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.2779 - val_loss: 0.2946\n",
      "Epoch 140/250\n",
      "10000/10000 [==============================] - 4s 446us/sample - loss: 0.2777 - val_loss: 0.2943\n",
      "Epoch 141/250\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 0.2774 - val_loss: 0.2940\n",
      "Epoch 142/250\n",
      "10000/10000 [==============================] - 5s 500us/sample - loss: 0.2771 - val_loss: 0.2937\n",
      "Epoch 143/250\n",
      "10000/10000 [==============================] - 5s 489us/sample - loss: 0.2768 - val_loss: 0.2934\n",
      "Epoch 144/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.2766 - val_loss: 0.2932\n",
      "Epoch 145/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.2763 - val_loss: 0.2929\n",
      "Epoch 146/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.2760 - val_loss: 0.2927\n",
      "Epoch 147/250\n",
      "10000/10000 [==============================] - 5s 500us/sample - loss: 0.2758 - val_loss: 0.2924\n",
      "Epoch 148/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2755 - val_loss: 0.2921\n",
      "Epoch 149/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 461us/sample - loss: 0.2753 - val_loss: 0.2919\n",
      "Epoch 150/250\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 0.2750 - val_loss: 0.2917\n",
      "Epoch 151/250\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 0.2748 - val_loss: 0.2914\n",
      "Epoch 152/250\n",
      "10000/10000 [==============================] - 5s 476us/sample - loss: 0.2745 - val_loss: 0.2912\n",
      "Epoch 153/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.2743 - val_loss: 0.2909\n",
      "Epoch 154/250\n",
      "10000/10000 [==============================] - 5s 485us/sample - loss: 0.2741 - val_loss: 0.2907\n",
      "Epoch 155/250\n",
      "10000/10000 [==============================] - 5s 463us/sample - loss: 0.2738 - val_loss: 0.2905\n",
      "Epoch 156/250\n",
      "10000/10000 [==============================] - 4s 447us/sample - loss: 0.2736 - val_loss: 0.2903\n",
      "Epoch 157/250\n",
      "10000/10000 [==============================] - 5s 456us/sample - loss: 0.2734 - val_loss: 0.2900\n",
      "Epoch 158/250\n",
      "10000/10000 [==============================] - 5s 454us/sample - loss: 0.2732 - val_loss: 0.2898\n",
      "Epoch 159/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2730 - val_loss: 0.2896\n",
      "Epoch 160/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2727 - val_loss: 0.2894\n",
      "Epoch 161/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.2725 - val_loss: 0.2892\n",
      "Epoch 162/250\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 0.2723 - val_loss: 0.2890\n",
      "Epoch 163/250\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 0.2721 - val_loss: 0.2888\n",
      "Epoch 164/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.2719 - val_loss: 0.2886\n",
      "Epoch 165/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.2717 - val_loss: 0.2884\n",
      "Epoch 166/250\n",
      "10000/10000 [==============================] - 5s 461us/sample - loss: 0.2715 - val_loss: 0.2882\n",
      "Epoch 167/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.2714 - val_loss: 0.2880\n",
      "Epoch 168/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.2712 - val_loss: 0.2879\n",
      "Epoch 169/250\n",
      "10000/10000 [==============================] - 4s 441us/sample - loss: 0.2710 - val_loss: 0.2877\n",
      "Epoch 170/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.2708 - val_loss: 0.2875\n",
      "Epoch 171/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2706 - val_loss: 0.2873\n",
      "Epoch 172/250\n",
      "10000/10000 [==============================] - 4s 442us/sample - loss: 0.2705 - val_loss: 0.2871\n",
      "Epoch 173/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2703 - val_loss: 0.2870\n",
      "Epoch 174/250\n",
      "10000/10000 [==============================] - 5s 467us/sample - loss: 0.2701 - val_loss: 0.2868\n",
      "Epoch 175/250\n",
      "10000/10000 [==============================] - 4s 446us/sample - loss: 0.2700 - val_loss: 0.2867\n",
      "Epoch 176/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.2698 - val_loss: 0.2865\n",
      "Epoch 177/250\n",
      "10000/10000 [==============================] - 4s 449us/sample - loss: 0.2696 - val_loss: 0.2863\n",
      "Epoch 178/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.2695 - val_loss: 0.2862\n",
      "Epoch 179/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.2693 - val_loss: 0.2860\n",
      "Epoch 180/250\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 0.2692 - val_loss: 0.2859\n",
      "Epoch 181/250\n",
      "10000/10000 [==============================] - 5s 454us/sample - loss: 0.2690 - val_loss: 0.2857\n",
      "Epoch 182/250\n",
      "10000/10000 [==============================] - 5s 480us/sample - loss: 0.2689 - val_loss: 0.2856\n",
      "Epoch 183/250\n",
      "10000/10000 [==============================] - 4s 441us/sample - loss: 0.2687 - val_loss: 0.2855\n",
      "Epoch 184/250\n",
      "10000/10000 [==============================] - 5s 456us/sample - loss: 0.2686 - val_loss: 0.2853\n",
      "Epoch 185/250\n",
      "10000/10000 [==============================] - 4s 446us/sample - loss: 0.2685 - val_loss: 0.2852\n",
      "Epoch 186/250\n",
      "10000/10000 [==============================] - 4s 446us/sample - loss: 0.2683 - val_loss: 0.2850\n",
      "Epoch 187/250\n",
      "10000/10000 [==============================] - 4s 450us/sample - loss: 0.2682 - val_loss: 0.2849\n",
      "Epoch 188/250\n",
      "10000/10000 [==============================] - 5s 463us/sample - loss: 0.2681 - val_loss: 0.2848\n",
      "Epoch 189/250\n",
      "10000/10000 [==============================] - 5s 465us/sample - loss: 0.2679 - val_loss: 0.2846\n",
      "Epoch 190/250\n",
      "10000/10000 [==============================] - 5s 472us/sample - loss: 0.2678 - val_loss: 0.2845\n",
      "Epoch 191/250\n",
      "10000/10000 [==============================] - 4s 446us/sample - loss: 0.2677 - val_loss: 0.2844\n",
      "Epoch 192/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.2676 - val_loss: 0.2843\n",
      "Epoch 193/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.2674 - val_loss: 0.2842\n",
      "Epoch 194/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2673 - val_loss: 0.2840\n",
      "Epoch 195/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2672 - val_loss: 0.2839\n",
      "Epoch 196/250\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 0.2671 - val_loss: 0.2838\n",
      "Epoch 197/250\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 0.2670 - val_loss: 0.2837\n",
      "Epoch 198/250\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 0.2669 - val_loss: 0.2836\n",
      "Epoch 199/250\n",
      "10000/10000 [==============================] - 5s 456us/sample - loss: 0.2668 - val_loss: 0.2835\n",
      "Epoch 200/250\n",
      "10000/10000 [==============================] - 5s 471us/sample - loss: 0.2667 - val_loss: 0.2834\n",
      "Epoch 201/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2666 - val_loss: 0.2833\n",
      "Epoch 202/250\n",
      "10000/10000 [==============================] - 5s 474us/sample - loss: 0.2665 - val_loss: 0.2832\n",
      "Epoch 203/250\n",
      "10000/10000 [==============================] - 5s 455us/sample - loss: 0.2664 - val_loss: 0.2831\n",
      "Epoch 204/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2663 - val_loss: 0.2830\n",
      "Epoch 205/250\n",
      "10000/10000 [==============================] - 4s 442us/sample - loss: 0.2662 - val_loss: 0.2829\n",
      "Epoch 206/250\n",
      "10000/10000 [==============================] - 5s 462us/sample - loss: 0.2661 - val_loss: 0.2828\n",
      "Epoch 207/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.2660 - val_loss: 0.2827\n",
      "Epoch 208/250\n",
      "10000/10000 [==============================] - 5s 482us/sample - loss: 0.2659 - val_loss: 0.2826\n",
      "Epoch 209/250\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 0.2658 - val_loss: 0.2825\n",
      "Epoch 210/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2657 - val_loss: 0.2825\n",
      "Epoch 211/250\n",
      "10000/10000 [==============================] - 5s 473us/sample - loss: 0.2656 - val_loss: 0.2824\n",
      "Epoch 212/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.2655 - val_loss: 0.2823\n",
      "Epoch 213/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2655 - val_loss: 0.2822\n",
      "Epoch 214/250\n",
      "10000/10000 [==============================] - 4s 444us/sample - loss: 0.2654 - val_loss: 0.2821\n",
      "Epoch 215/250\n",
      "10000/10000 [==============================] - 5s 458us/sample - loss: 0.2653 - val_loss: 0.2820\n",
      "Epoch 216/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.2652 - val_loss: 0.2820\n",
      "Epoch 217/250\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 0.2651 - val_loss: 0.2819\n",
      "Epoch 218/250\n",
      "10000/10000 [==============================] - 5s 459us/sample - loss: 0.2651 - val_loss: 0.2818\n",
      "Epoch 219/250\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 0.2650 - val_loss: 0.2817\n",
      "Epoch 220/250\n",
      "10000/10000 [==============================] - 5s 450us/sample - loss: 0.2649 - val_loss: 0.2817\n",
      "Epoch 221/250\n",
      "10000/10000 [==============================] - 5s 452us/sample - loss: 0.2648 - val_loss: 0.2816\n",
      "Epoch 222/250\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 0.2648 - val_loss: 0.2815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2647 - val_loss: 0.2815\n",
      "Epoch 224/250\n",
      "10000/10000 [==============================] - 5s 451us/sample - loss: 0.2646 - val_loss: 0.2814\n",
      "Epoch 225/250\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 0.2646 - val_loss: 0.2813\n",
      "Epoch 226/250\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 0.2645 - val_loss: 0.2813\n",
      "Epoch 227/250\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 0.2644 - val_loss: 0.2812\n",
      "Epoch 228/250\n",
      "10000/10000 [==============================] - 5s 457us/sample - loss: 0.2644 - val_loss: 0.2811\n",
      "Epoch 229/250\n",
      "10000/10000 [==============================] - 4s 450us/sample - loss: 0.2643 - val_loss: 0.2811\n",
      "Epoch 230/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.2643 - val_loss: 0.2810\n",
      "Epoch 231/250\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 0.2642 - val_loss: 0.2810\n",
      "Epoch 232/250\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 0.2641 - val_loss: 0.2809\n",
      "Epoch 233/250\n",
      "10000/10000 [==============================] - 5s 452us/sample - loss: 0.2641 - val_loss: 0.2809\n",
      "Epoch 234/250\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 0.2640 - val_loss: 0.2808\n",
      "Epoch 235/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.2640 - val_loss: 0.2807\n",
      "Epoch 236/250\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 0.2639 - val_loss: 0.2807\n",
      "Epoch 237/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.2639 - val_loss: 0.2806\n",
      "Epoch 238/250\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 0.2638 - val_loss: 0.2806\n",
      "Epoch 239/250\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 0.2638 - val_loss: 0.2805\n",
      "Epoch 240/250\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 0.2637 - val_loss: 0.2805\n",
      "Epoch 241/250\n",
      "10000/10000 [==============================] - 4s 450us/sample - loss: 0.2637 - val_loss: 0.2804\n",
      "Epoch 242/250\n",
      "10000/10000 [==============================] - 4s 439us/sample - loss: 0.2636 - val_loss: 0.2804\n",
      "Epoch 243/250\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 0.2636 - val_loss: 0.2804\n",
      "Epoch 244/250\n",
      "10000/10000 [==============================] - 5s 453us/sample - loss: 0.2635 - val_loss: 0.2803\n",
      "Epoch 245/250\n",
      "10000/10000 [==============================] - 5s 473us/sample - loss: 0.2635 - val_loss: 0.2803\n",
      "Epoch 246/250\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 0.2634 - val_loss: 0.2802\n",
      "Epoch 247/250\n",
      "10000/10000 [==============================] - 4s 434us/sample - loss: 0.2634 - val_loss: 0.2802\n",
      "Epoch 248/250\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 0.2633 - val_loss: 0.2801\n",
      "Epoch 249/250\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 0.2633 - val_loss: 0.2801\n",
      "Epoch 250/250\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 0.2633 - val_loss: 0.2800\n"
     ]
    }
   ],
   "source": [
    "result_spa = autoencoder.fit(X_train_noise, X_train, epochs=epochs, batch_size=batch_size, \n",
    "                shuffle=True, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3853996170043945,\n",
       " 0.3836963624954224,\n",
       " 0.38202127103805544,\n",
       " 0.38037659397125245,\n",
       " 0.37875699772834776,\n",
       " 0.37716308116912844,\n",
       " 0.37559936685562134,\n",
       " 0.3740597554683685,\n",
       " 0.37254601674079896,\n",
       " 0.3710579041481018,\n",
       " 0.3695930464744568,\n",
       " 0.3681502846240997,\n",
       " 0.36673199281692503,\n",
       " 0.36533606462478635,\n",
       " 0.36395331535339354,\n",
       " 0.36259328179359435,\n",
       " 0.36125131211280825,\n",
       " 0.35992978014945987,\n",
       " 0.35862258157730104,\n",
       " 0.35733432302474977,\n",
       " 0.35606333513259886,\n",
       " 0.35481020970344546,\n",
       " 0.3535728016853332,\n",
       " 0.35235345554351805,\n",
       " 0.35115041484832765,\n",
       " 0.34996576681137087,\n",
       " 0.34879623074531557,\n",
       " 0.34763786897659305,\n",
       " 0.34650133085250856,\n",
       " 0.3453714077949524,\n",
       " 0.34426118226051333,\n",
       " 0.343164639377594,\n",
       " 0.3420818314552307,\n",
       " 0.341016041135788,\n",
       " 0.3399609907627106,\n",
       " 0.3389195259094238,\n",
       " 0.33789250664710996,\n",
       " 0.33687637729644776,\n",
       " 0.335876868724823,\n",
       " 0.3348860373497009,\n",
       " 0.3339103850841522,\n",
       " 0.3329454431056976,\n",
       " 0.3319939807891846,\n",
       " 0.33105324277877807,\n",
       " 0.33012230644226076,\n",
       " 0.32920617303848265,\n",
       " 0.32829832048416135,\n",
       " 0.32740518288612364,\n",
       " 0.326520765209198,\n",
       " 0.3256464992046356,\n",
       " 0.3247831467151642,\n",
       " 0.3239287956237793,\n",
       " 0.32308488335609437,\n",
       " 0.3222532208442688,\n",
       " 0.32143071398735046,\n",
       " 0.3206184021949768,\n",
       " 0.3198149437904358,\n",
       " 0.3190187349319458,\n",
       " 0.3182351800918579,\n",
       " 0.3174597276210785,\n",
       " 0.31669403343200686,\n",
       " 0.3159357232570648,\n",
       " 0.3151859490871429,\n",
       " 0.3144477177143097,\n",
       " 0.31371878566741945,\n",
       " 0.31299337086677553,\n",
       " 0.31228031005859375,\n",
       " 0.31157368359565735,\n",
       " 0.3108751456260681,\n",
       " 0.3101868546009064,\n",
       " 0.3095072293281555,\n",
       " 0.3088306213855743,\n",
       " 0.3081638029575348,\n",
       " 0.3075046502113342,\n",
       " 0.3068553759098053,\n",
       " 0.3062116857051849,\n",
       " 0.3055738154888153,\n",
       " 0.304945268201828,\n",
       " 0.3043246985435486,\n",
       " 0.3037085258483887,\n",
       " 0.3031045698642731,\n",
       " 0.30250181126594544,\n",
       " 0.3019085579872131,\n",
       " 0.3013238621711731,\n",
       " 0.3007429396152496,\n",
       " 0.30016956653594973,\n",
       " 0.29960255155563353,\n",
       " 0.2990429206848145,\n",
       " 0.29848944520950316,\n",
       " 0.2979421007156372,\n",
       " 0.29740197496414184,\n",
       " 0.2968689276218414,\n",
       " 0.296341965675354,\n",
       " 0.2958168478965759,\n",
       " 0.29530243864059447,\n",
       " 0.29479165821075437,\n",
       " 0.29428821969032287,\n",
       " 0.2937895401954651,\n",
       " 0.29329749007225037,\n",
       " 0.2928114324569702,\n",
       " 0.29233272218704226,\n",
       " 0.2918569405555725,\n",
       " 0.29138978033065793,\n",
       " 0.2909246677875519,\n",
       " 0.2904665032863617,\n",
       " 0.290014439535141,\n",
       " 0.28956891622543335,\n",
       " 0.2891255696773529,\n",
       " 0.288689977645874,\n",
       " 0.2882598756313324,\n",
       " 0.28783555130958555,\n",
       " 0.2874152164459228,\n",
       " 0.2870019741535187,\n",
       " 0.28659209957122805,\n",
       " 0.28618727145195005,\n",
       " 0.2857887186527252,\n",
       " 0.28539651675224303,\n",
       " 0.2850060184955597,\n",
       " 0.28462231616973876,\n",
       " 0.284244139289856,\n",
       " 0.28387134881019593,\n",
       " 0.2835015471935272,\n",
       " 0.2831375750541687,\n",
       " 0.28277966747283934,\n",
       " 0.28242549743652345,\n",
       " 0.28207529067993165,\n",
       " 0.28173081135749817,\n",
       " 0.2813893238067627,\n",
       " 0.28105382595062256,\n",
       " 0.28072308950424196,\n",
       " 0.28039718770980837,\n",
       " 0.28007547841072084,\n",
       " 0.2797576769828796,\n",
       " 0.2794452220201492,\n",
       " 0.2791371162891388,\n",
       " 0.2788333643436432,\n",
       " 0.2785326482772827,\n",
       " 0.27823920288085935,\n",
       " 0.27794815554618835,\n",
       " 0.27766093640327455,\n",
       " 0.2773778175354004,\n",
       " 0.2770986919403076,\n",
       " 0.27682403831481933,\n",
       " 0.27655536136627196,\n",
       " 0.27628744463920596,\n",
       " 0.2760263811588287,\n",
       " 0.27576770963668823,\n",
       " 0.27551191215515136,\n",
       " 0.2752620913028717,\n",
       " 0.2750147856235504,\n",
       " 0.27477142519950865,\n",
       " 0.27453158025741575,\n",
       " 0.2742963906764984,\n",
       " 0.2740640175819397,\n",
       " 0.27383581151962283,\n",
       " 0.27360980973243715,\n",
       " 0.2733889536857605,\n",
       " 0.27316973929405214,\n",
       " 0.27295636148452757,\n",
       " 0.2727443182468414,\n",
       " 0.272536728143692,\n",
       " 0.27233232645988464,\n",
       " 0.27213074421882627,\n",
       " 0.2719335935115814,\n",
       " 0.2717400969028473,\n",
       " 0.2715483700275421,\n",
       " 0.2713597845554352,\n",
       " 0.27117497615814207,\n",
       " 0.2709931709289551,\n",
       " 0.2708140368938446,\n",
       " 0.27063819355964663,\n",
       " 0.27046579711437224,\n",
       " 0.270295800113678,\n",
       " 0.27012874579429624,\n",
       " 0.26996468868255613,\n",
       " 0.2698027201652527,\n",
       " 0.26964428143501284,\n",
       " 0.26948800082206725,\n",
       " 0.269334796667099,\n",
       " 0.26918300576210025,\n",
       " 0.26903499562740324,\n",
       " 0.26889086146354674,\n",
       " 0.26874674673080445,\n",
       " 0.26860650081634524,\n",
       " 0.26846865258216857,\n",
       " 0.2683324696302414,\n",
       " 0.26820004358291627,\n",
       " 0.2680670592308044,\n",
       " 0.2679384511470795,\n",
       " 0.26781141319274904,\n",
       " 0.26768747143745425,\n",
       " 0.26756532306671144,\n",
       " 0.26744617986679076,\n",
       " 0.26732818298339844,\n",
       " 0.2672132051944733,\n",
       " 0.2670982578754425,\n",
       " 0.2669864472866058,\n",
       " 0.2668772051334381,\n",
       " 0.26676936855316163,\n",
       " 0.2666635788917541,\n",
       " 0.26656000843048094,\n",
       " 0.266457234454155,\n",
       " 0.26635708136558534,\n",
       " 0.2662593069076538,\n",
       " 0.26616289472579957,\n",
       " 0.26606758546829223,\n",
       " 0.265974183511734,\n",
       " 0.265884134721756,\n",
       " 0.2657954512119293,\n",
       " 0.26570653882026674,\n",
       " 0.2656214464664459,\n",
       " 0.26553591709136964,\n",
       " 0.26545311160087587,\n",
       " 0.26537132434844973,\n",
       " 0.2652912986278534,\n",
       " 0.26521217212677,\n",
       " 0.26513555130958555,\n",
       " 0.26505939927101135,\n",
       " 0.2649851502418518,\n",
       " 0.26491262140274047,\n",
       " 0.2648412835597992,\n",
       " 0.26477033047676085,\n",
       " 0.264701743721962,\n",
       " 0.2646334933280945,\n",
       " 0.26456765832901,\n",
       " 0.2645028059482574,\n",
       " 0.26443902497291566,\n",
       " 0.2643757790565491,\n",
       " 0.2643155203819275,\n",
       " 0.2642546570777893,\n",
       " 0.2641954273223877,\n",
       " 0.26413813509941103,\n",
       " 0.26407903819084166,\n",
       " 0.26402436232566834,\n",
       " 0.26396966128349303,\n",
       " 0.26391623730659486,\n",
       " 0.2638634699821472,\n",
       " 0.2638111162185669,\n",
       " 0.2637610858440399,\n",
       " 0.26371096744537353,\n",
       " 0.26366132946014403,\n",
       " 0.2636130610466003,\n",
       " 0.26356658153533935,\n",
       " 0.2635202299118042,\n",
       " 0.26347503685951235,\n",
       " 0.2634312518596649,\n",
       " 0.2633866449832916,\n",
       " 0.2633436167001724,\n",
       " 0.2633017121315002,\n",
       " 0.2632608426570892]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_spa.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a86ecc6d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(  result_ucla.history['loss'], marker='', color='pink', linewidth=2, label='PCA')\n",
    "plt.plot(  result_sa.history['loss'], marker='', color='olive', linewidth=2, label = 'Stacked')\n",
    "plt.plot(  result_da.history['loss'], marker='', color='red', linewidth=2, label='Denoised')\n",
    "plt.plot(  result_spa.history['loss'], marker='', color='skyblue', linewidth=2, label='Spatial')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
